{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchio opencv-python-headless matplotlib prefetch_generator monai edt surface-distance medim\n",
    "!pip install nilearn scikit-image open3d scipy numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "import open3d as o3d\n",
    "\n",
    "def normalize_to_unit_cube(points):\n",
    "    \"\"\"\n",
    "    Normalizes a point cloud independently into the range [-1, 1] \n",
    "    as described in Section D.3.1.\n",
    "    \n",
    "    Args:\n",
    "        points (np.ndarray): (N, 3) array of points.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Normalized points.\n",
    "    \"\"\"\n",
    "    # Center the points\n",
    "    centroid = np.mean(points, axis=0)\n",
    "    points_centered = points - centroid\n",
    "    \n",
    "    # Scale to fit in [-1, 1]\n",
    "    # We find the maximum absolute distance from origin along any axis\n",
    "    max_dist = np.max(np.abs(points_centered))\n",
    "    \n",
    "    # Scale such that the bounding box is within [-1, 1]\n",
    "    if max_dist > 0:\n",
    "        points_normalized = points_centered / max_dist\n",
    "    else:\n",
    "        points_normalized = points_centered\n",
    "        \n",
    "    return points_normalized\n",
    "\n",
    "def apply_icp(source_points, target_points, threshold=0.02):\n",
    "    \"\"\"\n",
    "    Applies Iterative Closest Point (ICP) alignment.\n",
    "    \n",
    "    Args:\n",
    "        source_points (np.ndarray): Predicted points (to be aligned).\n",
    "        target_points (np.ndarray): Ground truth points (reference).\n",
    "        threshold (float): Max correspondence distance for ICP.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: The aligned source points.\n",
    "    \"\"\"\n",
    "    # Convert to Open3D point clouds\n",
    "    source_pcd = o3d.geometry.PointCloud()\n",
    "    source_pcd.points = o3d.utility.Vector3dVector(source_points)\n",
    "    \n",
    "    target_pcd = o3d.geometry.PointCloud()\n",
    "    target_pcd.points = o3d.utility.Vector3dVector(target_points)\n",
    "    \n",
    "    # Initial alignment (Identity)\n",
    "    trans_init = np.identity(4)\n",
    "    \n",
    "    # Apply Point-to-Point ICP\n",
    "    reg_p2p = o3d.pipelines.registration.registration_icp(\n",
    "        source_pcd, target_pcd, threshold, trans_init,\n",
    "        o3d.pipelines.registration.TransformationEstimationPointToPoint()\n",
    "    )\n",
    "    \n",
    "    # Transform the source points\n",
    "    source_pcd.transform(reg_p2p.transformation)\n",
    "    return np.asarray(source_pcd.points)\n",
    "\n",
    "def compute_f1_score_at_threshold(pred_points, gt_points, threshold=0.01):\n",
    "    \"\"\"\n",
    "    Computes F1 score at a specific threshold.\n",
    "    \n",
    "    Formula:\n",
    "    Precision = % of pred points within threshold of any gt point\n",
    "    Recall = % of gt points within threshold of any pred point\n",
    "    F1 = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "    \n",
    "    Args:\n",
    "        pred_points (np.ndarray): (N, 3) array of predicted points.\n",
    "        gt_points (np.ndarray): (M, 3) array of ground truth points.\n",
    "        threshold (float): Distance threshold (default 0.01 as per paper).\n",
    "        \n",
    "    Returns:\n",
    "        dict: Contains 'f1', 'precision', and 'recall'.\n",
    "    \"\"\"\n",
    "    # Build KD-Trees for efficient nearest neighbor search\n",
    "    # This is much faster than brute force distance calculation for 1M points\n",
    "    gt_tree = cKDTree(gt_points)\n",
    "    pred_tree = cKDTree(pred_points)\n",
    "    \n",
    "    # --- Compute Precision (Pred -> GT) ---\n",
    "    # For each point in Pred, find distance to nearest neighbor in GT\n",
    "    dist_pred_to_gt, _ = gt_tree.query(pred_points, k=1)\n",
    "    \n",
    "    # Precision: Percentage of pred points close enough to GT\n",
    "    precision = np.mean(dist_pred_to_gt < threshold)\n",
    "    \n",
    "    # --- Compute Recall (GT -> Pred) ---\n",
    "    # For each point in GT, find distance to nearest neighbor in Pred\n",
    "    dist_gt_to_pred, _ = pred_tree.query(gt_points, k=1)\n",
    "    \n",
    "    # Recall: Percentage of GT points close enough to Pred\n",
    "    recall = np.mean(dist_gt_to_pred < threshold)\n",
    "    \n",
    "    # --- Compute F1 Score (Harmonic Mean) ---\n",
    "    if precision + recall == 0:\n",
    "        f1 = 0.0\n",
    "    else:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        \n",
    "    return {\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }\n",
    "\n",
    "def evaluate_sam3d_metric(pred_points_raw, gt_points_raw):\n",
    "    \"\"\"\n",
    "    Full pipeline wrapper: Normalization -> ICP -> F1@0.01\n",
    "    \"\"\"\n",
    "    # 1. Normalize independently into [-1, 1]\n",
    "    pred_norm = normalize_to_unit_cube(pred_points_raw)\n",
    "    gt_norm = normalize_to_unit_cube(gt_points_raw)\n",
    "    \n",
    "    # 2. Apply ICP Alignment\n",
    "    # Note: ICP usually requires a rough initial alignment. \n",
    "    # Since we normalized to the same unit cube, they should be roughly at the same scale/center.\n",
    "    # However, rotations might still differ. This step handles fine-tuning.\n",
    "    pred_aligned = apply_icp(pred_norm, gt_norm)\n",
    "    \n",
    "    # 3. Compute Metrics\n",
    "    metrics = compute_f1_score_at_threshold(pred_aligned, gt_norm, threshold=0.01)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Prediction: test_data/amos_val_toy_data/imagesVa/amos_0013.nii.gz\n",
      "Processing Ground Truth: test_data/amos_val_toy_data/labelsVa/amos_0013.nii.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from nilearn import image\n",
    "from skimage import measure\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "# ==========================================\n",
    "# 1. Previous Metric Code (from previous answer)\n",
    "# ==========================================\n",
    "\n",
    "def normalize_to_unit_cube(points):\n",
    "    \"\"\"\n",
    "    Normalizes a point cloud independently into the range [-1, 1] \n",
    "    as described in Section D.3.1.\n",
    "    \"\"\"\n",
    "    centroid = np.mean(points, axis=0)\n",
    "    points_centered = points - centroid\n",
    "    max_dist = np.max(np.abs(points_centered))\n",
    "    if max_dist > 0:\n",
    "        points_normalized = points_centered / max_dist\n",
    "    else:\n",
    "        points_normalized = points_centered\n",
    "    return points_normalized\n",
    "\n",
    "def apply_icp(source_points, target_points, threshold=0.02):\n",
    "    \"\"\"\n",
    "    Applies ICP alignment as required by the methodology.\n",
    "    \"\"\"\n",
    "    source_pcd = o3d.geometry.PointCloud()\n",
    "    source_pcd.points = o3d.utility.Vector3dVector(source_points)\n",
    "    target_pcd = o3d.geometry.PointCloud()\n",
    "    target_pcd.points = o3d.utility.Vector3dVector(target_points)\n",
    "    \n",
    "    trans_init = np.identity(4)\n",
    "    reg_p2p = o3d.pipelines.registration.registration_icp(\n",
    "        source_pcd, target_pcd, threshold, trans_init,\n",
    "        o3d.pipelines.registration.TransformationEstimationPointToPoint()\n",
    "    )\n",
    "    source_pcd.transform(reg_p2p.transformation)\n",
    "    return np.asarray(source_pcd.points)\n",
    "\n",
    "def compute_f1_score(pred_points, gt_points, threshold=0.01):\n",
    "    \"\"\"\n",
    "    Computes F1 score using Precision and Recall.\n",
    "    \"\"\"\n",
    "    gt_tree = cKDTree(gt_points)\n",
    "    pred_tree = cKDTree(pred_points)\n",
    "    \n",
    "    # Precision: % of pred points within threshold of GT\n",
    "    dist_pred_to_gt, _ = gt_tree.query(pred_points, k=1)\n",
    "    precision = np.mean(dist_pred_to_gt < threshold)\n",
    "    \n",
    "    # Recall: % of GT points within threshold of pred\n",
    "    dist_gt_to_pred, _ = pred_tree.query(gt_points, k=1)\n",
    "    recall = np.mean(dist_gt_to_pred < threshold)\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        f1 = 0.0\n",
    "    else:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        \n",
    "    return f1, precision, recall\n",
    "\n",
    "# ==========================================\n",
    "# 2. New NIfTI Handling Code\n",
    "# ==========================================\n",
    "\n",
    "def nii_to_surface_points(nii_path, num_points=1000000, iso_value=0.5):\n",
    "    \"\"\"\n",
    "    Reads a NIfTI file, extracts the isosurface using Marching Cubes,\n",
    "    and uniformly samples points from that surface.\n",
    "    \n",
    "    Args:\n",
    "        nii_path (str): Path to the .nii.gz file.\n",
    "        num_points (int): Number of points to sample (Paper uses 1M ).\n",
    "        iso_value (float): Threshold for binary mask extraction.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: (N, 3) array of surface points in world coordinates.\n",
    "    \"\"\"\n",
    "    # 1. Load NIfTI file\n",
    "    img = image.load_img(nii_path)\n",
    "    data = img.get_fdata()\n",
    "    affine = img.affine\n",
    "\n",
    "    # 2. Extract Surface Mesh (Marching Cubes)\n",
    "    # This converts voxel data to a triangulated mesh (verts, faces)\n",
    "    # Step=1 ensures we check every voxel.\n",
    "    try:\n",
    "        verts_voxel, faces, _, _ = measure.marching_cubes(data, level=iso_value, step_size=1)\n",
    "    except ValueError:\n",
    "        print(f\"Error: No surface found in {nii_path} (is the image empty?)\")\n",
    "        return np.zeros((0, 3))\n",
    "\n",
    "    # 3. Apply Affine Transform (Voxel -> World Coordinates)\n",
    "    # We must respect voxel spacing (e.g., anisotropy) for accurate shape comparison.\n",
    "    # Add homogeneous coordinate (column of 1s)\n",
    "    ones = np.ones((verts_voxel.shape[0], 1))\n",
    "    verts_homo = np.hstack([verts_voxel, ones])\n",
    "    \n",
    "    # Multiply by affine matrix (transpose is needed for matrix multiplication order here)\n",
    "    verts_world = verts_homo @ affine.T\n",
    "    verts_world = verts_world[:, :3] # Drop homogeneous coordinate\n",
    "\n",
    "    # 4. Create Open3D Mesh for Uniform Sampling\n",
    "    mesh = o3d.geometry.TriangleMesh()\n",
    "    mesh.vertices = o3d.utility.Vector3dVector(verts_world)\n",
    "    mesh.triangles = o3d.utility.Vector3iVector(faces)\n",
    "    \n",
    "    # Ensure mesh is clean for sampling\n",
    "    mesh.compute_vertex_normals()\n",
    "    \n",
    "    # 5. Uniformly Sample Points\n",
    "    # This samples points based on face area, ensuring uniform coverage\n",
    "    pcd = mesh.sample_points_uniformly(number_of_points=num_points)\n",
    "    \n",
    "    return np.asarray(pcd.points)\n",
    "\n",
    "def evaluate_nifti_pair(pred_path, gt_path):\n",
    "    print(f\"Processing Prediction: {pred_path}\")\n",
    "    pred_points_raw = nii_to_surface_points(pred_path)\n",
    "    \n",
    "    print(f\"Processing Ground Truth: {gt_path}\")\n",
    "    gt_points_raw = nii_to_surface_points(gt_path)\n",
    "    \n",
    "    if len(pred_points_raw) == 0 or len(gt_points_raw) == 0:\n",
    "        print(\"Failed to extract surfaces.\")\n",
    "        return\n",
    "\n",
    "    # --- Apply Paper Methodology ---\n",
    "    \n",
    "    # 1. Normalize independently [-1, 1] \n",
    "    pred_norm = normalize_to_unit_cube(pred_points_raw)\n",
    "    gt_norm = normalize_to_unit_cube(gt_points_raw)\n",
    "    \n",
    "    # 2. Apply ICP Alignment \n",
    "    # (Threshold 0.05 allows for initial convergence, can be tightened)\n",
    "    pred_aligned = apply_icp(pred_norm, gt_norm, threshold=0.05)\n",
    "    \n",
    "    # 3. Compute Metrics (Threshold 0.01) [cite: 1152]\n",
    "    f1, prec, rec = compute_f1_score(pred_aligned, gt_norm, threshold=0.01)\n",
    "    \n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Evaluation Results:\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall:    {rec:.4f}\")\n",
    "    print(f\"F1@0.01:   {f1:.4f}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "\n",
    "scan_path = 'test_data/amos_val_toy_data/imagesVa/amos_0013.nii.gz'\n",
    "mask_path = 'test_data/amos_val_toy_data/labelsVa/amos_0013.nii.gz'\n",
    "\n",
    "evaluate_nifti_pair(scan_path, mask_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "Converting prediction volume to surface points...\n",
      "Loading test_data/amos_val_toy_data/imagesVa/amos_0013.nii.gz...\n",
      "Converting ground truth volume to surface points...\n",
      "Loading test_data/amos_val_toy_data/labelsVa/amos_0013.nii.gz...\n",
      "Normalizing...\n",
      "Aligning via ICP...\n",
      "Computing F1@0.01...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'f1': np.float64(0.060535171385332084),\n",
       " 'precision': np.float64(0.033162),\n",
       " 'recall': np.float64(0.346783)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nilearn.image\n",
    "from skimage.measure import marching_cubes\n",
    "import trimesh\n",
    "from scipy.spatial import cKDTree\n",
    "import open3d as o3d\n",
    "\n",
    "# --- 1. Reuse the Metric Code from the Previous Step ---\n",
    "\n",
    "def normalize_to_unit_cube(points):\n",
    "    \"\"\"Normalizes points to [-1, 1] as per[cite: 1149].\"\"\"\n",
    "    centroid = np.mean(points, axis=0)\n",
    "    points_centered = points - centroid\n",
    "    max_dist = np.max(np.abs(points_centered))\n",
    "    if max_dist > 0:\n",
    "        return points_centered / max_dist\n",
    "    return points_centered\n",
    "\n",
    "def apply_icp(source_points, target_points, threshold=0.02):\n",
    "    \"\"\"Aligns point clouds using ICP[cite: 1150].\"\"\"\n",
    "    source_pcd = o3d.geometry.PointCloud()\n",
    "    source_pcd.points = o3d.utility.Vector3dVector(source_points)\n",
    "    target_pcd = o3d.geometry.PointCloud()\n",
    "    target_pcd.points = o3d.utility.Vector3dVector(target_points)\n",
    "    \n",
    "    trans_init = np.identity(4)\n",
    "    reg_p2p = o3d.pipelines.registration.registration_icp(\n",
    "        source_pcd, target_pcd, threshold, trans_init,\n",
    "        o3d.pipelines.registration.TransformationEstimationPointToPoint()\n",
    "    )\n",
    "    source_pcd.transform(reg_p2p.transformation)\n",
    "    return np.asarray(source_pcd.points)\n",
    "\n",
    "def compute_f1_score_at_threshold(pred_points, gt_points, threshold=0.01):\n",
    "    \"\"\"Computes harmonic mean of precision and recall[cite: 1152].\"\"\"\n",
    "    gt_tree = cKDTree(gt_points)\n",
    "    pred_tree = cKDTree(pred_points)\n",
    "    \n",
    "    dist_pred_to_gt, _ = gt_tree.query(pred_points, k=1)\n",
    "    precision = np.mean(dist_pred_to_gt < threshold)\n",
    "    \n",
    "    dist_gt_to_pred, _ = pred_tree.query(gt_points, k=1)\n",
    "    recall = np.mean(dist_gt_to_pred < threshold)\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        f1 = 0.0\n",
    "    else:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        \n",
    "    return {\"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "# --- 2. New Code to Handle NIfTI Files ---\n",
    "\n",
    "def nifti_to_pointcloud(nii_path, num_samples=1_000_000, level=0.5):\n",
    "    \"\"\"\n",
    "    Reads a NIfTI file, extracts the surface mesh using marching cubes,\n",
    "    and uniformly samples points from that surface.\n",
    "    \"\"\"\n",
    "    # Load the image using nilearn\n",
    "    print(f\"Loading {nii_path}...\")\n",
    "    img = nilearn.image.load_img(nii_path)\n",
    "    data = img.get_fdata()\n",
    "    \n",
    "    # Ensure data is suitable for marching cubes (binary mask preferred)\n",
    "    # If the NIfTI contains probabilities, threshold it.\n",
    "    if np.max(data) <= 1.0 and np.max(data) > 0:\n",
    "         # Standardize mask to boolean\n",
    "        data = data > level\n",
    "    \n",
    "    # Extract surface mesh (vertices and faces)\n",
    "    # marching_cubes returns: verts, faces, normals, values\n",
    "    try:\n",
    "        verts, faces, _, _ = marching_cubes(data, level=level)\n",
    "    except RuntimeError:\n",
    "        print(f\"Warning: No surface found in {nii_path} at level {level}.\")\n",
    "        return np.zeros((num_samples, 3))\n",
    "\n",
    "    # Apply the affine transformation to map voxel coords to real-world coords\n",
    "    # NIfTI affine is a 4x4 matrix\n",
    "    affine = img.affine\n",
    "    # Homogeneous coordinates for vertices\n",
    "    verts_homogeneous = np.c_[verts, np.ones(verts.shape[0])]\n",
    "    # Apply affine: x_new = affine @ x_old\n",
    "    verts_transformed = (affine @ verts_homogeneous.T).T[:, :3]\n",
    "\n",
    "    # Create a Trimesh object for easy sampling\n",
    "    mesh = trimesh.Trimesh(vertices=verts_transformed, faces=faces)\n",
    "    \n",
    "    # Uniformly sample points from the mesh surface \n",
    "    samples, _ = trimesh.sample.sample_surface(mesh, num_samples)\n",
    "    \n",
    "    return samples\n",
    "\n",
    "def evaluate_nifti_pair(pred_path, gt_path):\n",
    "    \"\"\"\n",
    "    Full pipeline to evaluate two NIfTI files.\n",
    "    \"\"\"\n",
    "    # 1. Convert NIfTI volumes to Surface Point Clouds\n",
    "    # The paper specifies sampling 1M points \n",
    "    N = 1_000_000 \n",
    "    \n",
    "    print(\"Converting prediction volume to surface points...\")\n",
    "    pred_points = nifti_to_pointcloud(pred_path, num_samples=N)\n",
    "    \n",
    "    print(\"Converting ground truth volume to surface points...\")\n",
    "    gt_points = nifti_to_pointcloud(gt_path, num_samples=N)\n",
    "    \n",
    "    # Check if we got valid points\n",
    "    if len(pred_points) == 0 or len(gt_points) == 0:\n",
    "        print(\"Error: Empty point cloud generated.\")\n",
    "        return None\n",
    "\n",
    "    # 2. Normalize [cite: 1149]\n",
    "    print(\"Normalizing...\")\n",
    "    pred_norm = normalize_to_unit_cube(pred_points)\n",
    "    gt_norm = normalize_to_unit_cube(gt_points)\n",
    "    \n",
    "    # 3. Align [cite: 1150]\n",
    "    print(\"Aligning via ICP...\")\n",
    "    pred_aligned = apply_icp(pred_norm, gt_norm)\n",
    "    \n",
    "    # 4. Compute Metrics [cite: 1152]\n",
    "    print(\"Computing F1@0.01...\")\n",
    "    metrics = compute_f1_score_at_threshold(pred_aligned, gt_norm, threshold=0.01)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def compute_voxel_iou(point_cloud_1, point_cloud_2, resolution=64):\n",
    "    \"\"\"\n",
    "    Computes Voxel-IoU by voxelizing point clouds into a grid of specified resolution.\n",
    "    \n",
    "    Paper Description: \"We voxelize both point clouds to 64^3 resolution and \n",
    "    [cite_start]compute intersection-over-union over occupied voxels.\" [cite: 1155-1156]\n",
    "    \n",
    "    Args:\n",
    "        point_cloud_1 (np.ndarray): (N, 3) aligned/normalized points [-1, 1].\n",
    "        point_cloud_2 (np.ndarray): (M, 3) aligned/normalized points [-1, 1].\n",
    "        resolution (int): Grid resolution (default 64).\n",
    "        \n",
    "    Returns:\n",
    "        float: Voxel-IoU score.\n",
    "    \"\"\"\n",
    "    # Define the voxel grid range. Since points are normalized to [-1, 1],\n",
    "    # the grid should cover this range.\n",
    "    min_bound = np.array([-1.0, -1.0, -1.0])\n",
    "    max_bound = np.array([1.0, 1.0, 1.0])\n",
    "    \n",
    "    # Calculate voxel size\n",
    "    voxel_size = (max_bound - min_bound) / resolution\n",
    "    \n",
    "    def get_occupied_voxels(points):\n",
    "        # 1. Shift points to [0, 2] range (relative to min_bound)\n",
    "        points_shifted = points - min_bound\n",
    "        \n",
    "        # 2. Divide by voxel size to get fractional indices\n",
    "        indices_float = points_shifted / voxel_size\n",
    "        \n",
    "        # 3. Floor to get integer indices\n",
    "        indices = np.floor(indices_float).astype(int)\n",
    "        \n",
    "        # 4. Clip indices to ensure they are within [0, resolution-1]\n",
    "        # (Handles potential floating point errors at the exact boundaries)\n",
    "        indices = np.clip(indices, 0, resolution - 1)\n",
    "        \n",
    "        # 5. Get unique indices (set of occupied voxels)\n",
    "        # We turn the Nx3 array into a set of tuples for efficient set operations\n",
    "        occupied_set = set(map(tuple, indices))\n",
    "        return occupied_set\n",
    "\n",
    "    # Get sets of occupied voxel coordinates for both shapes\n",
    "    voxels_1 = get_occupied_voxels(point_cloud_1)\n",
    "    voxels_2 = get_occupied_voxels(point_cloud_2)\n",
    "    \n",
    "    # Compute Intersection over Union\n",
    "    intersection = len(voxels_1.intersection(voxels_2))\n",
    "    union = len(voxels_1.union(voxels_2))\n",
    "    \n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "        \n",
    "    return intersection / union\n",
    "\n",
    "# ==========================================\n",
    "# Updated Evaluation Function\n",
    "# ==========================================\n",
    "\n",
    "def evaluate_nifti_pair_complete(pred_path, gt_path):\n",
    "    print(f\"Processing Prediction: {pred_path}\")\n",
    "    pred_points_raw = nii_to_surface_points(pred_path)\n",
    "    \n",
    "    print(f\"Processing Ground Truth: {gt_path}\")\n",
    "    gt_points_raw = nii_to_surface_points(gt_path)\n",
    "    \n",
    "    if len(pred_points_raw) == 0 or len(gt_points_raw) == 0:\n",
    "        print(\"Failed to extract surfaces.\")\n",
    "        return\n",
    "\n",
    "    # 1. Normalize independently [-1, 1]\n",
    "    pred_norm = normalize_to_unit_cube(pred_points_raw)\n",
    "    gt_norm = normalize_to_unit_cube(gt_points_raw)\n",
    "    \n",
    "    # 2. Apply ICP Alignment\n",
    "    print(\"Aligning meshes...\")\n",
    "    pred_aligned = apply_icp(pred_norm, gt_norm, threshold=0.05)\n",
    "    \n",
    "    # 3. Compute Metrics\n",
    "    print(\"Computing F1 Score...\")\n",
    "    f1, prec, rec = compute_f1_score(pred_aligned, gt_norm, threshold=0.01)\n",
    "    \n",
    "    print(\"Computing Voxel-IoU...\")\n",
    "    # Note: We use the *aligned* prediction and the *normalized* ground truth\n",
    "    # because Voxel-IoU is sensitive to alignment errors.\n",
    "    iou = compute_voxel_iou(pred_aligned, gt_norm, resolution=64)\n",
    "    \n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Evaluation Results:\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall:    {rec:.4f}\")\n",
    "    print(f\"F1@0.01:   {f1:.4f}\")\n",
    "    print(f\"Voxel-IoU: {iou:.4f}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# --- Example Usage ---\n",
    "scan_path = 'test_data/amos_val_toy_data/imagesVa/amos_0013.nii.gz'\n",
    "mask_path = 'test_data/amos_val_toy_data/labelsVa/amos_0013.nii.gz'\n",
    "\n",
    "evaluate_nifti_pair_complete(scan_path, mask_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files...\n",
      "Found 15 objects in Ground Truth: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
      "\n",
      "--- Evaluating Object ID: 1 ---\n",
      "  F1@0.01:   0.0289\n",
      "  Voxel-IoU: 0.0514\n",
      "\n",
      "--- Evaluating Object ID: 2 ---\n",
      "  F1@0.01:   0.0236\n",
      "  Voxel-IoU: 0.0227\n",
      "\n",
      "--- Evaluating Object ID: 3 ---\n",
      "  F1@0.01:   0.0541\n",
      "  Voxel-IoU: 0.0882\n",
      "\n",
      "--- Evaluating Object ID: 4 ---\n",
      "  F1@0.01:   0.0363\n",
      "  Voxel-IoU: 0.0535\n",
      "\n",
      "--- Evaluating Object ID: 5 ---\n",
      "  F1@0.01:   0.0539\n",
      "  Voxel-IoU: 0.0605\n",
      "\n",
      "--- Evaluating Object ID: 6 ---\n",
      "  F1@0.01:   0.0687\n",
      "  Voxel-IoU: 0.0794\n",
      "\n",
      "--- Evaluating Object ID: 7 ---\n",
      "  F1@0.01:   0.0439\n",
      "  Voxel-IoU: 0.0627\n",
      "\n",
      "--- Evaluating Object ID: 8 ---\n",
      "  F1@0.01:   0.0538\n",
      "  Voxel-IoU: 0.0318\n",
      "\n",
      "--- Evaluating Object ID: 9 ---\n",
      "  F1@0.01:   0.0613\n",
      "  Voxel-IoU: 0.0353\n",
      "\n",
      "--- Evaluating Object ID: 10 ---\n",
      "  F1@0.01:   0.0612\n",
      "  Voxel-IoU: 0.0714\n",
      "\n",
      "--- Evaluating Object ID: 11 ---\n",
      "  F1@0.01:   0.0585\n",
      "  Voxel-IoU: 0.0652\n",
      "\n",
      "--- Evaluating Object ID: 12 ---\n",
      "  F1@0.01:   0.0532\n",
      "  Voxel-IoU: 0.0763\n",
      "\n",
      "--- Evaluating Object ID: 13 ---\n",
      "  F1@0.01:   0.0503\n",
      "  Voxel-IoU: 0.0676\n",
      "\n",
      "--- Evaluating Object ID: 14 ---\n",
      "  F1@0.01:   0.0391\n",
      "  Voxel-IoU: 0.0507\n",
      "\n",
      "--- Evaluating Object ID: 15 ---\n",
      "  F1@0.01:   0.0506\n",
      "  Voxel-IoU: 0.0661\n",
      "\n",
      "==============================\n",
      "FINAL AGGREGATE RESULTS (Mean over 15 objects)\n",
      "Mean F1@0.01:   0.0492\n",
      "Mean Voxel-IoU: 0.0589\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from nilearn import image\n",
    "from skimage import measure\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "# ==========================================\n",
    "# 1. Metric Helper Functions (Unchanged)\n",
    "# ==========================================\n",
    "\n",
    "def normalize_to_unit_cube(points):\n",
    "    if len(points) == 0: return points\n",
    "    centroid = np.mean(points, axis=0)\n",
    "    points_centered = points - centroid\n",
    "    max_dist = np.max(np.abs(points_centered))\n",
    "    return points_centered / max_dist if max_dist > 0 else points_centered\n",
    "\n",
    "def apply_icp(source_points, target_points, threshold=0.02):\n",
    "    if len(source_points) == 0 or len(target_points) == 0:\n",
    "        return source_points\n",
    "        \n",
    "    source_pcd = o3d.geometry.PointCloud()\n",
    "    source_pcd.points = o3d.utility.Vector3dVector(source_points)\n",
    "    target_pcd = o3d.geometry.PointCloud()\n",
    "    target_pcd.points = o3d.utility.Vector3dVector(target_points)\n",
    "    \n",
    "    trans_init = np.identity(4)\n",
    "    reg_p2p = o3d.pipelines.registration.registration_icp(\n",
    "        source_pcd, target_pcd, threshold, trans_init,\n",
    "        o3d.pipelines.registration.TransformationEstimationPointToPoint()\n",
    "    )\n",
    "    source_pcd.transform(reg_p2p.transformation)\n",
    "    return np.asarray(source_pcd.points)\n",
    "\n",
    "def compute_f1_score(pred_points, gt_points, threshold=0.01):\n",
    "    if len(pred_points) == 0 or len(gt_points) == 0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "        \n",
    "    gt_tree = cKDTree(gt_points)\n",
    "    pred_tree = cKDTree(pred_points)\n",
    "    \n",
    "    dist_pred_to_gt, _ = gt_tree.query(pred_points, k=1)\n",
    "    precision = np.mean(dist_pred_to_gt < threshold)\n",
    "    \n",
    "    dist_gt_to_pred, _ = pred_tree.query(gt_points, k=1)\n",
    "    recall = np.mean(dist_gt_to_pred < threshold)\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        return 0.0, precision, recall\n",
    "        \n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1, precision, recall\n",
    "\n",
    "def compute_voxel_iou(point_cloud_1, point_cloud_2, resolution=64):\n",
    "    if len(point_cloud_1) == 0 and len(point_cloud_2) == 0: return 1.0 # Both empty\n",
    "    if len(point_cloud_1) == 0 or len(point_cloud_2) == 0: return 0.0  # One empty\n",
    "    \n",
    "    min_bound = np.array([-1.0, -1.0, -1.0])\n",
    "    max_bound = np.array([1.0, 1.0, 1.0])\n",
    "    voxel_size = (max_bound - min_bound) / resolution\n",
    "    \n",
    "    def get_voxels(points):\n",
    "        indices = np.floor((points - min_bound) / voxel_size).astype(int)\n",
    "        indices = np.clip(indices, 0, resolution - 1)\n",
    "        return set(map(tuple, indices))\n",
    "\n",
    "    voxels_1 = get_voxels(point_cloud_1)\n",
    "    voxels_2 = get_voxels(point_cloud_2)\n",
    "    \n",
    "    intersection = len(voxels_1.intersection(voxels_2))\n",
    "    union = len(voxels_1.union(voxels_2))\n",
    "    \n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "# ==========================================\n",
    "# 2. Updated Multi-Label NIfTI Handling\n",
    "# ==========================================\n",
    "\n",
    "def extract_surface_for_label(data, affine, label_id, num_points=100000):\n",
    "    \"\"\"\n",
    "    Extracts surface points SPECIFICALLY for a given integer label ID.\n",
    "    \"\"\"\n",
    "    # Create binary mask for just this object\n",
    "    # Note: 'level=0.5' on a boolean mask works to extract the boundary\n",
    "    binary_mask = (data == label_id).astype(float)\n",
    "    \n",
    "    if np.sum(binary_mask) == 0:\n",
    "        return np.zeros((0, 3))\n",
    "\n",
    "    try:\n",
    "        # Extract mesh from the binary mask of this specific label\n",
    "        verts_voxel, faces, _, _ = measure.marching_cubes(binary_mask, level=0.5, step_size=1)\n",
    "        \n",
    "        # Apply affine transform to world coordinates\n",
    "        ones = np.ones((verts_voxel.shape[0], 1))\n",
    "        verts_world = (np.hstack([verts_voxel, ones]) @ affine.T)[:, :3]\n",
    "\n",
    "        # Sample points\n",
    "        mesh = o3d.geometry.TriangleMesh()\n",
    "        mesh.vertices = o3d.utility.Vector3dVector(verts_world)\n",
    "        mesh.triangles = o3d.utility.Vector3iVector(faces)\n",
    "        mesh.compute_vertex_normals()\n",
    "        \n",
    "        pcd = mesh.sample_points_uniformly(number_of_points=num_points)\n",
    "        return np.asarray(pcd.points)\n",
    "        \n",
    "    except (ValueError, RuntimeError):\n",
    "        # Can happen if object is too small (single voxel)\n",
    "        return np.zeros((0, 3))\n",
    "\n",
    "def evaluate_multi_object_nifti(pred_path, gt_path):\n",
    "    print(f\"Loading files...\")\n",
    "    pred_img = image.load_img(pred_path)\n",
    "    gt_img = image.load_img(gt_path)\n",
    "    \n",
    "    pred_data = pred_img.get_fdata()\n",
    "    gt_data = gt_img.get_fdata()\n",
    "    \n",
    "    # Identify all unique object IDs in Ground Truth (excluding background 0)\n",
    "    # We assume we want to evaluate every object present in the GT.\n",
    "    gt_labels = np.unique(gt_data)\n",
    "    gt_labels = gt_labels[gt_labels != 0].astype(int)\n",
    "    \n",
    "    print(f\"Found {len(gt_labels)} objects in Ground Truth: {gt_labels}\")\n",
    "    \n",
    "    results = {\n",
    "        \"f1\": [],\n",
    "        \"iou\": [],\n",
    "        \"precision\": [],\n",
    "        \"recall\": []\n",
    "    }\n",
    "    \n",
    "    # Iterate over each object (1 to N)\n",
    "    for label_id in gt_labels:\n",
    "        print(f\"\\n--- Evaluating Object ID: {label_id} ---\")\n",
    "        \n",
    "        # 1. Extract Surfaces for this specific ID\n",
    "        # Note: We use the SAME label_id for pred and gt, assuming the\n",
    "        # segmentation classes are consistent.\n",
    "        gt_points = extract_surface_for_label(gt_data, gt_img.affine, label_id)\n",
    "        pred_points = extract_surface_for_label(pred_data, pred_img.affine, label_id)\n",
    "        \n",
    "        if len(gt_points) == 0:\n",
    "            print(f\"Skipping Object {label_id} (GT empty/too small)\")\n",
    "            continue\n",
    "            \n",
    "        if len(pred_points) == 0:\n",
    "            print(f\"Object {label_id} missing in prediction!\")\n",
    "            results[\"f1\"].append(0.0)\n",
    "            results[\"iou\"].append(0.0)\n",
    "            results[\"precision\"].append(0.0)\n",
    "            results[\"recall\"].append(0.0)\n",
    "            continue\n",
    "            \n",
    "        # 2. Normalize & Align (Per-Object)\n",
    "        gt_norm = normalize_to_unit_cube(gt_points)\n",
    "        pred_norm = normalize_to_unit_cube(pred_points)\n",
    "        \n",
    "        # Align this specific object\n",
    "        pred_aligned = apply_icp(pred_norm, gt_norm, threshold=0.05)\n",
    "        \n",
    "        # 3. Compute Metrics for this object\n",
    "        f1, prec, rec = compute_f1_score(pred_aligned, gt_norm, threshold=0.01)\n",
    "        iou = compute_voxel_iou(pred_aligned, gt_norm)\n",
    "        \n",
    "        print(f\"  F1@0.01:   {f1:.4f}\")\n",
    "        print(f\"  Voxel-IoU: {iou:.4f}\")\n",
    "        \n",
    "        results[\"f1\"].append(f1)\n",
    "        results[\"iou\"].append(iou)\n",
    "        results[\"precision\"].append(prec)\n",
    "        results[\"recall\"].append(rec)\n",
    "    \n",
    "    # === Aggregate Results ===\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(f\"FINAL AGGREGATE RESULTS (Mean over {len(results['f1'])} objects)\")\n",
    "    print(f\"Mean F1@0.01:   {np.mean(results['f1']):.4f}\")\n",
    "    print(f\"Mean Voxel-IoU: {np.mean(results['iou']):.4f}\")\n",
    "    print(\"=\"*30)\n",
    "\n",
    "# --- Example Usage ---\n",
    "scan_path = 'test_data/amos_val_toy_data/imagesVa/amos_0013.nii.gz'\n",
    "mask_path = 'test_data/amos_val_toy_data/labelsVa/amos_0013.nii.gz'\n",
    "\n",
    "evaluate_multi_object_nifti(scan_path, mask_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'evaluate_multi_object_nifti' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 217\u001b[39m\n\u001b[32m    214\u001b[39m scan_path = \u001b[33m'\u001b[39m\u001b[33mtest_data/amos_val_toy_data/imagesVa/amos_0013.nii.gz\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    215\u001b[39m mask_path = \u001b[33m'\u001b[39m\u001b[33mtest_data/amos_val_toy_data/labelsVa/amos_0013.nii.gz\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m \u001b[43mevaluate_multi_object_nifti\u001b[49m(scan_path, mask_path)\n",
      "\u001b[31mNameError\u001b[39m: name 'evaluate_multi_object_nifti' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from nilearn import image\n",
    "from skimage import measure\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# ==========================================\n",
    "# 1. Metric Helper Functions\n",
    "# ==========================================\n",
    "\n",
    "def normalize_to_unit_cube(points):\n",
    "    \"\"\"Normalizes point cloud to [-1, 1] range.\"\"\"\n",
    "    if len(points) == 0: return points\n",
    "    centroid = np.mean(points, axis=0)\n",
    "    points_centered = points - centroid\n",
    "    max_dist = np.max(np.abs(points_centered))\n",
    "    return points_centered / max_dist if max_dist > 0 else points_centered\n",
    "\n",
    "def apply_icp(source_points, target_points, threshold=0.02):\n",
    "    \"\"\"Aligns source points to target points using ICP.\"\"\"\n",
    "    if len(source_points) == 0 or len(target_points) == 0:\n",
    "        return source_points\n",
    "    \n",
    "    source_pcd = o3d.geometry.PointCloud()\n",
    "    source_pcd.points = o3d.utility.Vector3dVector(source_points)\n",
    "    target_pcd = o3d.geometry.PointCloud()\n",
    "    target_pcd.points = o3d.utility.Vector3dVector(target_points)\n",
    "    \n",
    "    trans_init = np.identity(4)\n",
    "    reg_p2p = o3d.pipelines.registration.registration_icp(\n",
    "        source_pcd, target_pcd, threshold, trans_init,\n",
    "        o3d.pipelines.registration.TransformationEstimationPointToPoint()\n",
    "    )\n",
    "    source_pcd.transform(reg_p2p.transformation)\n",
    "    return np.asarray(source_pcd.points)\n",
    "\n",
    "def compute_f1_score(pred_points, gt_points, threshold=0.01):\n",
    "    \"\"\"Computes F1@0.01 score.\"\"\"\n",
    "    if len(pred_points) == 0 or len(gt_points) == 0: return 0.0, 0.0, 0.0\n",
    "    \n",
    "    gt_tree = cKDTree(gt_points)\n",
    "    pred_tree = cKDTree(pred_points)\n",
    "    \n",
    "    dist_pred_to_gt, _ = gt_tree.query(pred_points, k=1)\n",
    "    precision = np.mean(dist_pred_to_gt < threshold)\n",
    "    \n",
    "    dist_gt_to_pred, _ = pred_tree.query(gt_points, k=1)\n",
    "    recall = np.mean(dist_gt_to_pred < threshold)\n",
    "    \n",
    "    if precision + recall == 0: return 0.0, precision, recall\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1, precision, recall\n",
    "\n",
    "def compute_voxel_iou(point_cloud_1, point_cloud_2, resolution=64):\n",
    "    \"\"\"Computes Voxel-IoU at 64^3 resolution.\"\"\"\n",
    "    if len(point_cloud_1) == 0 and len(point_cloud_2) == 0: return 1.0\n",
    "    if len(point_cloud_1) == 0 or len(point_cloud_2) == 0: return 0.0\n",
    "    \n",
    "    min_bound = np.array([-1.0, -1.0, -1.0])\n",
    "    max_bound = np.array([1.0, 1.0, 1.0])\n",
    "    voxel_size = (max_bound - min_bound) / resolution\n",
    "    \n",
    "    def get_voxels(points):\n",
    "        indices = np.floor((points - min_bound) / voxel_size).astype(int)\n",
    "        indices = np.clip(indices, 0, resolution - 1)\n",
    "        return set(map(tuple, indices))\n",
    "\n",
    "    voxels_1 = get_voxels(point_cloud_1)\n",
    "    voxels_2 = get_voxels(point_cloud_2)\n",
    "    \n",
    "    intersection = len(voxels_1.intersection(voxels_2))\n",
    "    union = len(voxels_1.union(voxels_2))\n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "def compute_chamfer_distance(pred_points, gt_points):\n",
    "    \"\"\"Computes Chamfer Distance (CD).\"\"\"\n",
    "    if len(pred_points) == 0 or len(gt_points) == 0: return float('inf')\n",
    "    \n",
    "    gt_tree = cKDTree(gt_points)\n",
    "    pred_tree = cKDTree(pred_points)\n",
    "    \n",
    "    dist_pred_to_gt, _ = gt_tree.query(pred_points, k=1)\n",
    "    dist_gt_to_pred, _ = pred_tree.query(gt_points, k=1)\n",
    "    \n",
    "    return np.mean(dist_pred_to_gt) + np.mean(dist_gt_to_pred)\n",
    "\n",
    "def compute_emd(pred_points, gt_points, max_points=2048):\n",
    "    \"\"\"\n",
    "    Computes Earth Mover's Distance (EMD).\n",
    "    \n",
    "    Paper Definition: \"Quantifies the minimal cost required to transport one point \n",
    "    distribution to match the other... enforcing bijective correspondence\" [cite: 1158-1159].\n",
    "    \n",
    "    Note: Exact EMD is O(N^3). We downsample to `max_points` to keep this tractable on CPU.\n",
    "    \"\"\"\n",
    "    if len(pred_points) == 0 or len(gt_points) == 0:\n",
    "        return float('inf')\n",
    "\n",
    "    # 1. Downsample for tractability\n",
    "    # EMD requires |S1| == |S2| for a bijection. We assume equal resampling.\n",
    "    n_points = min(len(pred_points), len(gt_points), max_points)\n",
    "    \n",
    "    # Random choice without replacement to maintain distribution properties\n",
    "    # (Setting seed for reproducibility)\n",
    "    np.random.seed(42)\n",
    "    indices_pred = np.random.choice(len(pred_points), n_points, replace=False)\n",
    "    indices_gt = np.random.choice(len(gt_points), n_points, replace=False)\n",
    "    \n",
    "    pred_sampled = pred_points[indices_pred]\n",
    "    gt_sampled = gt_points[indices_gt]\n",
    "    \n",
    "    # 2. Compute Distance Matrix (Cost Matrix)\n",
    "    # Shape: (N, N)\n",
    "    dists = cdist(pred_sampled, gt_sampled, metric='euclidean')\n",
    "    \n",
    "    # 3. Solve Linear Sum Assignment (Hungarian Algorithm)\n",
    "    # Finds row_ind, col_ind such that sum(dists[row, col]) is minimized\n",
    "    row_ind, col_ind = linear_sum_assignment(dists)\n",
    "    \n",
    "    # 4. Compute Mean Cost\n",
    "    # \"Quantifies the minimal cost\" [cite: 1158] -> usually averaged by N\n",
    "    total_cost = dists[row_ind, col_ind].sum()\n",
    "    emd_value = total_cost / n_points\n",
    "    \n",
    "    return emd_value\n",
    "\n",
    "# ==========================================\n",
    "# 2. Multi-Label Evaluation Pipeline\n",
    "# ==========================================\n",
    "\n",
    "def extract_surface_for_label(data, affine, label_id, num_points=100000):\n",
    "    \"\"\"Extracts surface points for a specific label ID.\"\"\"\n",
    "    binary_mask = (data == label_id).astype(float)\n",
    "    if np.sum(binary_mask) == 0: return np.zeros((0, 3))\n",
    "\n",
    "    try:\n",
    "        verts_voxel, faces, _, _ = measure.marching_cubes(binary_mask, level=0.5, step_size=1)\n",
    "        ones = np.ones((verts_voxel.shape[0], 1))\n",
    "        verts_world = (np.hstack([verts_voxel, ones]) @ affine.T)[:, :3]\n",
    "        \n",
    "        mesh = o3d.geometry.TriangleMesh()\n",
    "        mesh.vertices = o3d.utility.Vector3dVector(verts_world)\n",
    "        mesh.triangles = o3d.utility.Vector3iVector(faces)\n",
    "        mesh.compute_vertex_normals()\n",
    "        \n",
    "        pcd = mesh.sample_points_uniformly(number_of_points=num_points)\n",
    "        return np.asarray(pcd.points)\n",
    "    except (ValueError, RuntimeError):\n",
    "        return np.zeros((0, 3))\n",
    "\n",
    "def evaluate_all_metrics(pred_path, gt_path):\n",
    "    print(f\"Loading files...\")\n",
    "    pred_img = image.load_img(pred_path)\n",
    "    gt_img = image.load_img(gt_path)\n",
    "    \n",
    "    pred_data = pred_img.get_fdata()\n",
    "    gt_data = gt_img.get_fdata()\n",
    "    \n",
    "    gt_labels = np.unique(gt_data)\n",
    "    gt_labels = gt_labels[gt_labels != 0].astype(int)\n",
    "    \n",
    "    results = {\"f1\": [], \"iou\": [], \"chamfer\": [], \"emd\": []}\n",
    "    \n",
    "    for label_id in gt_labels:\n",
    "        print(f\"\\n--- Object ID: {label_id} ---\")\n",
    "        \n",
    "        # 1. Extraction (High resolution for F1/CD)\n",
    "        gt_points = extract_surface_for_label(gt_data, gt_img.affine, label_id, num_points=10000)\n",
    "        pred_points = extract_surface_for_label(pred_data, pred_img.affine, label_id, num_points=10000)\n",
    "        \n",
    "        # Handle Missing/Empty\n",
    "        if len(gt_points) == 0: continue\n",
    "        if len(pred_points) == 0:\n",
    "            results[\"f1\"].append(0.0)\n",
    "            results[\"iou\"].append(0.0)\n",
    "            results[\"chamfer\"].append(1.0) # Penalty\n",
    "            results[\"emd\"].append(1.0)     # Penalty\n",
    "            continue\n",
    "            \n",
    "        # 2. Normalize & Align\n",
    "        gt_norm = normalize_to_unit_cube(gt_points)\n",
    "        pred_norm = normalize_to_unit_cube(pred_points)\n",
    "        pred_aligned = apply_icp(pred_norm, gt_norm, threshold=0.05)\n",
    "        \n",
    "        # 3. Compute Metrics\n",
    "        f1, _, _ = compute_f1_score(pred_aligned, gt_norm)\n",
    "        iou = compute_voxel_iou(pred_aligned, gt_norm)\n",
    "        chamfer = compute_chamfer_distance(pred_aligned, gt_norm)\n",
    "        \n",
    "        # EMD (Uses internal downsampling to e.g., 2048 points)\n",
    "        emd = compute_emd(pred_aligned, gt_norm, max_points=2048)\n",
    "        \n",
    "        print(f\"  F1@0.01:   {f1:.4f}\")\n",
    "        print(f\"  Voxel-IoU: {iou:.4f}\")\n",
    "        print(f\"  Chamfer:   {chamfer:.4f}\")\n",
    "        print(f\"  EMD:       {emd:.4f}\")\n",
    "        \n",
    "        results[\"f1\"].append(f1)\n",
    "        results[\"iou\"].append(iou)\n",
    "        results[\"chamfer\"].append(chamfer)\n",
    "        results[\"emd\"].append(emd)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(f\"FINAL MEAN SCORES\")\n",
    "    print(f\"Mean F1@0.01:   {np.mean(results['f1']):.4f}\")\n",
    "    print(f\"Mean Voxel-IoU: {np.mean(results['iou']):.4f}\")\n",
    "    print(f\"Mean Chamfer:   {np.mean(results['chamfer']):.4f}\")\n",
    "    print(f\"Mean EMD:       {np.mean(results['emd']):.4f}\")\n",
    "    print(\"=\"*30)\n",
    "\n",
    "# --- Example Usage ---\n",
    "scan_path = 'test_data/amos_val_toy_data/imagesVa/amos_0013.nii.gz'\n",
    "mask_path = 'test_data/amos_val_toy_data/labelsVa/amos_0013.nii.gz'\n",
    "\n",
    "evaluate_all_metrics(scan_path, mask_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from nilearn import image\n",
    "from skimage import measure\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import os\n",
    "\n",
    "# ==========================================\n",
    "# 1. Metric Helper Functions (Unchanged)\n",
    "# ==========================================\n",
    "\n",
    "def normalize_to_unit_cube(points):\n",
    "    \"\"\"Normalizes point cloud to [-1, 1] range.\"\"\"\n",
    "    if len(points) == 0: return points\n",
    "    centroid = np.mean(points, axis=0)\n",
    "    points_centered = points - centroid\n",
    "    max_dist = np.max(np.abs(points_centered))\n",
    "    return points_centered / max_dist if max_dist > 0 else points_centered\n",
    "\n",
    "def apply_icp(source_points, target_points, threshold=0.02):\n",
    "    \"\"\"Aligns source points to target points using ICP.\"\"\"\n",
    "    if len(source_points) == 0 or len(target_points) == 0:\n",
    "        return source_points\n",
    "    \n",
    "    source_pcd = o3d.geometry.PointCloud()\n",
    "    source_pcd.points = o3d.utility.Vector3dVector(source_points)\n",
    "    target_pcd = o3d.geometry.PointCloud()\n",
    "    target_pcd.points = o3d.utility.Vector3dVector(target_points)\n",
    "    \n",
    "    trans_init = np.identity(4)\n",
    "    reg_p2p = o3d.pipelines.registration.registration_icp(\n",
    "        source_pcd, target_pcd, threshold, trans_init,\n",
    "        o3d.pipelines.registration.TransformationEstimationPointToPoint()\n",
    "    )\n",
    "    source_pcd.transform(reg_p2p.transformation)\n",
    "    return np.asarray(source_pcd.points)\n",
    "\n",
    "def compute_f1_score(pred_points, gt_points, threshold=0.01):\n",
    "    \"\"\"Computes F1@0.01 score.\"\"\"\n",
    "    if len(pred_points) == 0 or len(gt_points) == 0: return 0.0, 0.0, 0.0\n",
    "    \n",
    "    gt_tree = cKDTree(gt_points)\n",
    "    pred_tree = cKDTree(pred_points)\n",
    "    \n",
    "    dist_pred_to_gt, _ = gt_tree.query(pred_points, k=1)\n",
    "    precision = np.mean(dist_pred_to_gt < threshold)\n",
    "    \n",
    "    dist_gt_to_pred, _ = pred_tree.query(gt_points, k=1)\n",
    "    recall = np.mean(dist_gt_to_pred < threshold)\n",
    "    \n",
    "    if precision + recall == 0: return 0.0, precision, recall\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1, precision, recall\n",
    "\n",
    "def compute_voxel_iou(point_cloud_1, point_cloud_2, resolution=64):\n",
    "    \"\"\"Computes Voxel-IoU at 64^3 resolution.\"\"\"\n",
    "    if len(point_cloud_1) == 0 and len(point_cloud_2) == 0: return 1.0\n",
    "    if len(point_cloud_1) == 0 or len(point_cloud_2) == 0: return 0.0\n",
    "    \n",
    "    min_bound = np.array([-1.0, -1.0, -1.0])\n",
    "    max_bound = np.array([1.0, 1.0, 1.0])\n",
    "    voxel_size = (max_bound - min_bound) / resolution\n",
    "    \n",
    "    def get_voxels(points):\n",
    "        indices = np.floor((points - min_bound) / voxel_size).astype(int)\n",
    "        indices = np.clip(indices, 0, resolution - 1)\n",
    "        return set(map(tuple, indices))\n",
    "\n",
    "    voxels_1 = get_voxels(point_cloud_1)\n",
    "    voxels_2 = get_voxels(point_cloud_2)\n",
    "    \n",
    "    intersection = len(voxels_1.intersection(voxels_2))\n",
    "    union = len(voxels_1.union(voxels_2))\n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "def compute_chamfer_distance(pred_points, gt_points):\n",
    "    \"\"\"Computes Chamfer Distance (CD).\"\"\"\n",
    "    if len(pred_points) == 0 or len(gt_points) == 0: return float('inf')\n",
    "    \n",
    "    gt_tree = cKDTree(gt_points)\n",
    "    pred_tree = cKDTree(pred_points)\n",
    "    \n",
    "    dist_pred_to_gt, _ = gt_tree.query(pred_points, k=1)\n",
    "    dist_gt_to_pred, _ = pred_tree.query(gt_points, k=1)\n",
    "    \n",
    "    return np.mean(dist_pred_to_gt) + np.mean(dist_gt_to_pred)\n",
    "\n",
    "def compute_emd(pred_points, gt_points, max_points=2048):\n",
    "    \"\"\"Computes Earth Mover's Distance (EMD).\"\"\"\n",
    "    if len(pred_points) == 0 or len(gt_points) == 0:\n",
    "        return float('inf')\n",
    "\n",
    "    n_points = min(len(pred_points), len(gt_points), max_points)\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    indices_pred = np.random.choice(len(pred_points), n_points, replace=False)\n",
    "    indices_gt = np.random.choice(len(gt_points), n_points, replace=False)\n",
    "    \n",
    "    pred_sampled = pred_points[indices_pred]\n",
    "    gt_sampled = gt_points[indices_gt]\n",
    "    \n",
    "    dists = cdist(pred_sampled, gt_sampled, metric='euclidean')\n",
    "    row_ind, col_ind = linear_sum_assignment(dists)\n",
    "    \n",
    "    total_cost = dists[row_ind, col_ind].sum()\n",
    "    emd_value = total_cost / n_points\n",
    "    \n",
    "    return emd_value\n",
    "\n",
    "# ==========================================\n",
    "# 2. Data Extraction Helpers (Modified)\n",
    "# ==========================================\n",
    "\n",
    "def extract_surface_from_nii(nii_path, label_id, num_points=10000):\n",
    "    \"\"\"Extracts surface points from NIfTI using Marching Cubes.\"\"\"\n",
    "    img = image.load_img(nii_path)\n",
    "    data = img.get_fdata()\n",
    "    affine = img.affine\n",
    "    \n",
    "    binary_mask = (data == label_id).astype(float)\n",
    "    if np.sum(binary_mask) == 0: \n",
    "        print(f\"Warning: Label {label_id} not found in NIfTI.\")\n",
    "        return np.zeros((0, 3))\n",
    "\n",
    "    try:\n",
    "        # Extract surface mesh\n",
    "        verts_voxel, faces, _, _ = measure.marching_cubes(binary_mask, level=0.5, step_size=1)\n",
    "        # Apply affine to move to world coordinates\n",
    "        ones = np.ones((verts_voxel.shape[0], 1))\n",
    "        verts_world = (np.hstack([verts_voxel, ones]) @ affine.T)[:, :3]\n",
    "        \n",
    "        # Convert to Open3D to sample points uniformly\n",
    "        mesh = o3d.geometry.TriangleMesh()\n",
    "        mesh.vertices = o3d.utility.Vector3dVector(verts_world)\n",
    "        mesh.triangles = o3d.utility.Vector3iVector(faces)\n",
    "        mesh.compute_vertex_normals()\n",
    "        \n",
    "        pcd = mesh.sample_points_uniformly(number_of_points=num_points)\n",
    "        return np.asarray(pcd.points)\n",
    "    except (ValueError, RuntimeError) as e:\n",
    "        print(f\"Extraction Error: {e}\")\n",
    "        return np.zeros((0, 3))\n",
    "\n",
    "def extract_surface_from_ply(ply_path, num_points=10000):\n",
    "    \"\"\"Loads a PLY file and samples points uniformly.\"\"\"\n",
    "    if not os.path.exists(ply_path):\n",
    "        print(f\"Error: PLY file not found at {ply_path}\")\n",
    "        return np.zeros((0, 3))\n",
    "        \n",
    "    try:\n",
    "        # Try loading as a Mesh first (most likely for surfaces)\n",
    "        mesh = o3d.io.read_triangle_mesh(ply_path)\n",
    "        if len(mesh.triangles) > 0:\n",
    "            pcd = mesh.sample_points_uniformly(number_of_points=num_points)\n",
    "            return np.asarray(pcd.points)\n",
    "        else:\n",
    "            # If no triangles, load as PointCloud\n",
    "            pcd = o3d.io.read_point_cloud(ply_path)\n",
    "            # If point cloud is huge, downsample; if small, return as is\n",
    "            # Ideally, we resample to num_points to match density\n",
    "            if len(pcd.points) > 0:\n",
    "                # Simple random downsampling if too large\n",
    "                if len(pcd.points) > num_points:\n",
    "                    pcd = pcd.random_down_sample(num_points / len(pcd.points))\n",
    "                return np.asarray(pcd.points)\n",
    "            return np.zeros((0, 3))\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"PLY Loading Error: {e}\")\n",
    "        return np.zeros((0, 3))\n",
    "\n",
    "# ==========================================\n",
    "# 3. Main Comparison Function\n",
    "# ==========================================\n",
    "\n",
    "def evaluate_nii_vs_ply(nii_path, ply_path, target_label_id=1):\n",
    "    \"\"\"\n",
    "    Compares a specific object in a NIfTI file against a PLY mesh.\n",
    "    \n",
    "    Args:\n",
    "        nii_path: Path to segmentation mask (.nii.gz)\n",
    "        ply_path: Path to surface mesh (.ply)\n",
    "        target_label_id: The integer label in the NIfTI file to compare.\n",
    "    \"\"\"\n",
    "    print(f\"Comparing NIfTI Label {target_label_id} vs PLY file...\")\n",
    "    \n",
    "    # 1. Extraction\n",
    "    print(\"Extracting NIfTI surface...\")\n",
    "    nii_points = extract_surface_from_nii(nii_path, target_label_id, num_points=10000)\n",
    "    \n",
    "    print(\"Loading PLY surface...\")\n",
    "    ply_points = extract_surface_from_ply(ply_path, num_points=10000)\n",
    "    \n",
    "    # Handle Missing/Empty\n",
    "    if len(nii_points) == 0 or len(ply_points) == 0:\n",
    "        print(\"Error: One of the inputs yielded 0 points.\")\n",
    "        return\n",
    "        \n",
    "    # 2. Normalize & Align\n",
    "    # Assuming PLY is Ground Truth (GT) and NIfTI is Prediction (or vice versa)\n",
    "    # We normalize both to unit cube to make metrics scale-invariant\n",
    "    gt_norm = normalize_to_unit_cube(ply_points)\n",
    "    pred_norm = normalize_to_unit_cube(nii_points)\n",
    "    \n",
    "    # Apply ICP to handle slight rotational/translational misalignments\n",
    "    # warning: if coordinate systems are completely different (e.g. RAS vs LPS),\n",
    "    # ICP might not be enough.\n",
    "    pred_aligned = apply_icp(pred_norm, gt_norm, threshold=0.05)\n",
    "    \n",
    "    # 3. Compute Metrics\n",
    "    print(\"\\nComputing Metrics...\")\n",
    "    f1, precision, recall = compute_f1_score(pred_aligned, gt_norm)\n",
    "    iou = compute_voxel_iou(pred_aligned, gt_norm)\n",
    "    chamfer = compute_chamfer_distance(pred_aligned, gt_norm)\n",
    "    emd = compute_emd(pred_aligned, gt_norm, max_points=2048)\n",
    "    \n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Results for NIfTI Label {target_label_id} vs PLY\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"  F1@0.01:   {f1:.4f}\")\n",
    "    print(f\"  Voxel-IoU: {iou:.4f}\")\n",
    "    print(f\"  Chamfer:   {chamfer:.4f}\")\n",
    "    print(f\"  EMD:       {emd:.4f}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# --- Example Usage ---\n",
    "mask_file = 'segmentation.nii.gz'\n",
    "mesh_file = 'ground_truth.ply'\n",
    "    \n",
    "# Run comparison for label 1 inside the NIfTI file\n",
    "evaluate_nii_vs_ply(mask_file, mesh_file, target_label_id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "Comparing Prediction PLY vs Ground Truth PLY...\n",
      "Loading Prediction surface...\n",
      "\u001b[1;33m[Open3D WARNING] geometry::TriangleMesh appears to be a geometry::PointCloud (only contains vertices, but no triangles).\u001b[0;m\n",
      "Loading Ground Truth surface...\n",
      "\u001b[1;33m[Open3D WARNING] geometry::TriangleMesh appears to be a geometry::PointCloud (only contains vertices, but no triangles).\u001b[0;m\n",
      "Loaded points -> Pred: 10000, GT: 10000\n",
      "Applying ICP alignment...\n",
      "\n",
      "Computing Metrics...\n",
      "------------------------------\n",
      "Results: Prediction vs Ground Truth\n",
      "------------------------------\n",
      "  F1@0.01:   0.1463\n",
      "  Voxel-IoU: 0.2661\n",
      "  Voxel-Dice:0.4204\n",
      "  Chamfer:   0.0812\n",
      "  EMD:       0.0919\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import os\n",
    "\n",
    "# ==========================================\n",
    "# 1. Metric Helper Functions (Preserved)\n",
    "# ==========================================\n",
    "\n",
    "def normalize_to_unit_cube(points):\n",
    "    \"\"\"Normalizes point cloud to [-1, 1] range.\"\"\"\n",
    "    if len(points) == 0: return points\n",
    "    centroid = np.mean(points, axis=0)\n",
    "    points_centered = points - centroid\n",
    "    max_dist = np.max(np.abs(points_centered))\n",
    "    return points_centered / max_dist if max_dist > 0 else points_centered\n",
    "\n",
    "def apply_icp(source_points, target_points, threshold=0.02):\n",
    "    \"\"\"Aligns source points to target points using ICP.\"\"\"\n",
    "    if len(source_points) == 0 or len(target_points) == 0:\n",
    "        return source_points\n",
    "    \n",
    "    source_pcd = o3d.geometry.PointCloud()\n",
    "    source_pcd.points = o3d.utility.Vector3dVector(source_points)\n",
    "    target_pcd = o3d.geometry.PointCloud()\n",
    "    target_pcd.points = o3d.utility.Vector3dVector(target_points)\n",
    "    \n",
    "    trans_init = np.identity(4)\n",
    "    reg_p2p = o3d.pipelines.registration.registration_icp(\n",
    "        source_pcd, target_pcd, threshold, trans_init,\n",
    "        o3d.pipelines.registration.TransformationEstimationPointToPoint()\n",
    "    )\n",
    "    source_pcd.transform(reg_p2p.transformation)\n",
    "    return np.asarray(source_pcd.points)\n",
    "\n",
    "def compute_f1_score(pred_points, gt_points, threshold=0.01):\n",
    "    \"\"\"Computes F1@0.01 score.\"\"\"\n",
    "    if len(pred_points) == 0 or len(gt_points) == 0: return 0.0, 0.0, 0.0\n",
    "    \n",
    "    gt_tree = cKDTree(gt_points)\n",
    "    pred_tree = cKDTree(pred_points)\n",
    "    \n",
    "    dist_pred_to_gt, _ = gt_tree.query(pred_points, k=1)\n",
    "    precision = np.mean(dist_pred_to_gt < threshold)\n",
    "    \n",
    "    dist_gt_to_pred, _ = pred_tree.query(gt_points, k=1)\n",
    "    recall = np.mean(dist_gt_to_pred < threshold)\n",
    "    \n",
    "    if precision + recall == 0: return 0.0, precision, recall\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1, precision, recall\n",
    "\n",
    "def compute_voxel_iou(point_cloud_1, point_cloud_2, resolution=64):\n",
    "    \"\"\"Computes Voxel-IoU at 64^3 resolution.\"\"\"\n",
    "    if len(point_cloud_1) == 0 and len(point_cloud_2) == 0: return 1.0\n",
    "    if len(point_cloud_1) == 0 or len(point_cloud_2) == 0: return 0.0\n",
    "    \n",
    "    min_bound = np.array([-1.0, -1.0, -1.0])\n",
    "    max_bound = np.array([1.0, 1.0, 1.0])\n",
    "    voxel_size = (max_bound - min_bound) / resolution\n",
    "    \n",
    "    def get_voxels(points):\n",
    "        indices = np.floor((points - min_bound) / voxel_size).astype(int)\n",
    "        indices = np.clip(indices, 0, resolution - 1)\n",
    "        return set(map(tuple, indices))\n",
    "\n",
    "    voxels_1 = get_voxels(point_cloud_1)\n",
    "    voxels_2 = get_voxels(point_cloud_2)\n",
    "    \n",
    "    intersection = len(voxels_1.intersection(voxels_2))\n",
    "    union = len(voxels_1.union(voxels_2))\n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "def compute_voxel_dice(point_cloud_1, point_cloud_2, resolution=64):\n",
    "    \"\"\"Computes Voxel-Dice at 64^3 resolution.\"\"\"\n",
    "    if len(point_cloud_1) == 0 and len(point_cloud_2) == 0: return 1.0\n",
    "    if len(point_cloud_1) == 0 or len(point_cloud_2) == 0: return 0.0\n",
    "    \n",
    "    min_bound = np.array([-1.0, -1.0, -1.0])\n",
    "    max_bound = np.array([1.0, 1.0, 1.0])\n",
    "    voxel_size = (max_bound - min_bound) / resolution\n",
    "    \n",
    "    def get_voxels(points):\n",
    "        indices = np.floor((points - min_bound) / voxel_size).astype(int)\n",
    "        indices = np.clip(indices, 0, resolution - 1)\n",
    "        return set(map(tuple, indices))\n",
    "\n",
    "    voxels_1 = get_voxels(point_cloud_1)\n",
    "    voxels_2 = get_voxels(point_cloud_2)\n",
    "    \n",
    "    intersection = len(voxels_1.intersection(voxels_2))\n",
    "    denom = len(voxels_1) + len(voxels_2)\n",
    "    \n",
    "    return (2.0 * intersection) / denom if denom > 0 else 0.0\n",
    "\n",
    "def compute_chamfer_distance(pred_points, gt_points):\n",
    "    \"\"\"Computes Chamfer Distance (CD).\"\"\"\n",
    "    if len(pred_points) == 0 or len(gt_points) == 0: return float('inf')\n",
    "    \n",
    "    gt_tree = cKDTree(gt_points)\n",
    "    pred_tree = cKDTree(pred_points)\n",
    "    \n",
    "    dist_pred_to_gt, _ = gt_tree.query(pred_points, k=1)\n",
    "    dist_gt_to_pred, _ = pred_tree.query(gt_points, k=1)\n",
    "    \n",
    "    return np.mean(dist_pred_to_gt) + np.mean(dist_gt_to_pred)\n",
    "\n",
    "def compute_emd(pred_points, gt_points, max_points=2048):\n",
    "    \"\"\"Computes Earth Mover's Distance (EMD).\"\"\"\n",
    "    if len(pred_points) == 0 or len(gt_points) == 0:\n",
    "        return float('inf')\n",
    "\n",
    "    n_points = min(len(pred_points), len(gt_points), max_points)\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    indices_pred = np.random.choice(len(pred_points), n_points, replace=False)\n",
    "    indices_gt = np.random.choice(len(gt_points), n_points, replace=False)\n",
    "    \n",
    "    pred_sampled = pred_points[indices_pred]\n",
    "    gt_sampled = gt_points[indices_gt]\n",
    "    \n",
    "    dists = cdist(pred_sampled, gt_sampled, metric='euclidean')\n",
    "    row_ind, col_ind = linear_sum_assignment(dists)\n",
    "    \n",
    "    total_cost = dists[row_ind, col_ind].sum()\n",
    "    emd_value = total_cost / n_points\n",
    "    \n",
    "    return emd_value\n",
    "\n",
    "# ==========================================\n",
    "# 2. Data Extraction Helpers (Streamlined)\n",
    "# ==========================================\n",
    "\n",
    "def load_and_sample_ply(ply_input, num_points=10000):\n",
    "    \"\"\"\n",
    "    Loads a PLY file (or accepts an Open3D object) and samples points uniformly.\n",
    "    \"\"\"\n",
    "    # 1. Handle string path input\n",
    "    if isinstance(ply_input, str):\n",
    "        if not os.path.exists(ply_input):\n",
    "            print(f\"Error: PLY file not found at {ply_input}\")\n",
    "            return np.zeros((0, 3))\n",
    "        \n",
    "        try:\n",
    "            # Try loading as a Mesh first (most likely for surfaces)\n",
    "            mesh = o3d.io.read_triangle_mesh(ply_input)\n",
    "            if len(mesh.triangles) > 0:\n",
    "                pcd = mesh.sample_points_uniformly(number_of_points=num_points)\n",
    "                return np.asarray(pcd.points)\n",
    "            else:\n",
    "                # If no triangles, load as PointCloud\n",
    "                pcd = o3d.io.read_point_cloud(ply_input)\n",
    "                # Downsample if needed\n",
    "                if len(pcd.points) > num_points:\n",
    "                    # Random downsample returns a point cloud, we grab indices or just use points\n",
    "                    # open3d random_down_sample ratio is float 0-1\n",
    "                    ratio = num_points / len(pcd.points)\n",
    "                    pcd = pcd.random_down_sample(ratio)\n",
    "                return np.asarray(pcd.points)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"PLY Loading Error: {e}\")\n",
    "            return np.zeros((0, 3))\n",
    "\n",
    "    # 2. Handle existing Open3D objects (in-memory)\n",
    "    elif isinstance(ply_input, (o3d.geometry.TriangleMesh, o3d.geometry.PointCloud)):\n",
    "        try:\n",
    "            if isinstance(ply_input, o3d.geometry.TriangleMesh):\n",
    "                pcd = ply_input.sample_points_uniformly(number_of_points=num_points)\n",
    "                return np.asarray(pcd.points)\n",
    "            else:\n",
    "                # It is a PointCloud\n",
    "                if len(ply_input.points) > num_points:\n",
    "                    ratio = num_points / len(ply_input.points)\n",
    "                    pcd = ply_input.random_down_sample(ratio)\n",
    "                    return np.asarray(pcd.points)\n",
    "                return np.asarray(ply_input.points)\n",
    "        except Exception as e:\n",
    "            print(f\"Object Processing Error: {e}\")\n",
    "            return np.zeros((0, 3))\n",
    "            \n",
    "    else:\n",
    "        print(\"Error: Input must be a file path string or an Open3D geometry object.\")\n",
    "        return np.zeros((0, 3))\n",
    "\n",
    "# ==========================================\n",
    "# 3. Main Comparison Function\n",
    "# ==========================================\n",
    "\n",
    "def evaluate_ply_vs_ply(pred_ply, gt_ply, num_points=10000, do_icp=True):\n",
    "    \"\"\"\n",
    "    Compares two PLY inputs (Prediction vs Ground Truth).\n",
    "    \n",
    "    Args:\n",
    "        pred_ply: Path to prediction .ply file OR Open3D geometry object.\n",
    "        gt_ply: Path to ground truth .ply file OR Open3D geometry object.\n",
    "        num_points: Number of points to sample for evaluation.\n",
    "        do_icp: Boolean to enable/disable ICP alignment.\n",
    "    \"\"\"\n",
    "    print(f\"Comparing Prediction PLY vs Ground Truth PLY...\")\n",
    "    \n",
    "    # 1. Extraction\n",
    "    print(\"Loading Prediction surface...\")\n",
    "    pred_points = load_and_sample_ply(pred_ply, num_points=num_points)\n",
    "    \n",
    "    print(\"Loading Ground Truth surface...\")\n",
    "    gt_points = load_and_sample_ply(gt_ply, num_points=num_points)\n",
    "    \n",
    "    # Handle Missing/Empty\n",
    "    if len(pred_points) == 0 or len(gt_points) == 0:\n",
    "        print(\"Error: One of the inputs yielded 0 points.\")\n",
    "        return\n",
    "        \n",
    "    print(f\"Loaded points -> Pred: {len(pred_points)}, GT: {len(gt_points)}\")\n",
    "\n",
    "    # 2. Normalize & Align\n",
    "    # Normalize both to unit cube to make metrics scale-invariant\n",
    "    gt_norm = normalize_to_unit_cube(gt_points)\n",
    "    pred_norm = normalize_to_unit_cube(pred_points)\n",
    "    \n",
    "    if do_icp:\n",
    "        print(\"Applying ICP alignment...\")\n",
    "        pred_aligned = apply_icp(pred_norm, gt_norm, threshold=0.05)\n",
    "    else:\n",
    "        print(\"Skipping ICP...\")\n",
    "        pred_aligned = pred_norm\n",
    "    \n",
    "    # 3. Compute Metrics\n",
    "    print(\"\\nComputing Metrics...\")\n",
    "    f1, precision, recall = compute_f1_score(pred_aligned, gt_norm)\n",
    "    iou = compute_voxel_iou(pred_aligned, gt_norm)\n",
    "    dice = compute_voxel_dice(pred_aligned, gt_norm)\n",
    "    chamfer = compute_chamfer_distance(pred_aligned, gt_norm)\n",
    "    emd = compute_emd(pred_aligned, gt_norm, max_points=2048)\n",
    "    \n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Results: Prediction vs Ground Truth\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"  F1@0.01:   {f1:.4f}\")\n",
    "    print(f\"  Voxel-IoU: {iou:.4f}\")\n",
    "    print(f\"  Voxel-Dice:{dice:.4f}\")\n",
    "    print(f\"  Chamfer:   {chamfer:.4f}\")\n",
    "    print(f\"  EMD:       {emd:.4f}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# --- Example Usage ---\n",
    "# if __name__ == \"__main__\":\n",
    "pred_file = '/PHShome/yl535/project/python/sam_3d/sam-3d-objects/results/gaussians/RET002OD_cup.ply'\n",
    "gt_file = '/PHShome/yl535/project/python/sam_3d/sam-3d-objects/results/gaussians/RET002OD_disc.ply'\n",
    "\n",
    "# Ensure files exist before running example, or just run the function\n",
    "evaluate_ply_vs_ply(pred_file, gt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "# ==========================================\n",
    "# 1. Metric Helper Functions\n",
    "# ==========================================\n",
    "\n",
    "def normalize_to_unit_cube(points):\n",
    "    \"\"\"Normalizes point cloud to [-1, 1] range.\"\"\"\n",
    "    if len(points) == 0: return points\n",
    "    centroid = np.mean(points, axis=0)\n",
    "    points_centered = points - centroid\n",
    "    max_dist = np.max(np.abs(points_centered))\n",
    "    return points_centered / max_dist if max_dist > 0 else points_centered\n",
    "\n",
    "def apply_icp(source_points, target_points, threshold=0.02):\n",
    "    \"\"\"Aligns source points to target points using ICP.\"\"\"\n",
    "    if len(source_points) == 0 or len(target_points) == 0:\n",
    "        return source_points\n",
    "    \n",
    "    source_pcd = o3d.geometry.PointCloud()\n",
    "    source_pcd.points = o3d.utility.Vector3dVector(source_points)\n",
    "    target_pcd = o3d.geometry.PointCloud()\n",
    "    target_pcd.points = o3d.utility.Vector3dVector(target_points)\n",
    "    \n",
    "    trans_init = np.identity(4)\n",
    "    # Using point-to-point for general robustness without normals\n",
    "    reg_p2p = o3d.pipelines.registration.registration_icp(\n",
    "        source_pcd, target_pcd, threshold, trans_init,\n",
    "        o3d.pipelines.registration.TransformationEstimationPointToPoint()\n",
    "    )\n",
    "    source_pcd.transform(reg_p2p.transformation)\n",
    "    return np.asarray(source_pcd.points)\n",
    "\n",
    "def compute_metrics_pair(p1, p2, threshold=0.01, resolution=64, max_emd_points=2048):\n",
    "    \"\"\"\n",
    "    Computes all 5 metrics for a single pair of point clouds (normalized).\n",
    "    Returns a dictionary of results.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # --- F1 Score ---\n",
    "    gt_tree = cKDTree(p2)\n",
    "    pred_tree = cKDTree(p1)\n",
    "    \n",
    "    dist_p1_to_p2, _ = gt_tree.query(p1, k=1)\n",
    "    precision = np.mean(dist_p1_to_p2 < threshold)\n",
    "    \n",
    "    dist_p2_to_p1, _ = pred_tree.query(p2, k=1)\n",
    "    recall = np.mean(dist_p2_to_p1 < threshold)\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        results['f1'] = 0.0\n",
    "    else:\n",
    "        results['f1'] = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    # --- Voxel IoU & Dice ---\n",
    "    min_bound = np.array([-1.0, -1.0, -1.0])\n",
    "    voxel_size = 2.0 / resolution # range [-1, 1] is size 2\n",
    "    \n",
    "    def get_voxels(points):\n",
    "        indices = np.floor((points - min_bound) / voxel_size).astype(int)\n",
    "        indices = np.clip(indices, 0, resolution - 1)\n",
    "        return set(map(tuple, indices))\n",
    "\n",
    "    voxels_1 = get_voxels(p1)\n",
    "    voxels_2 = get_voxels(p2)\n",
    "    \n",
    "    intersection = len(voxels_1.intersection(voxels_2))\n",
    "    union = len(voxels_1.union(voxels_2))\n",
    "    denom = len(voxels_1) + len(voxels_2)\n",
    "    \n",
    "    results['iou'] = intersection / union if union > 0 else 0.0\n",
    "    results['dice'] = (2.0 * intersection) / denom if denom > 0 else 0.0\n",
    "\n",
    "    # --- Chamfer Distance ---\n",
    "    # (Symmetric Chamfer: mean(d(p1->p2)) + mean(d(p2->p1)))\n",
    "    results['chamfer'] = np.mean(dist_p1_to_p2) + np.mean(dist_p2_to_p1)\n",
    "\n",
    "    # --- EMD ---\n",
    "    n_points = min(len(p1), len(p2), max_emd_points)\n",
    "    # Simple random sampling for EMD speed\n",
    "    # Note: For strict determinism, set seed inside or pass seed\n",
    "    idx1 = np.random.choice(len(p1), n_points, replace=False)\n",
    "    idx2 = np.random.choice(len(p2), n_points, replace=False)\n",
    "    \n",
    "    dists = cdist(p1[idx1], p2[idx2], metric='euclidean')\n",
    "    row_ind, col_ind = linear_sum_assignment(dists)\n",
    "    results['emd'] = dists[row_ind, col_ind].sum() / n_points\n",
    "\n",
    "    return results\n",
    "\n",
    "# ==========================================\n",
    "# 2. Data Loading Helpers\n",
    "# ==========================================\n",
    "\n",
    "def load_and_process_ply(path, num_points=10000):\n",
    "    \"\"\"Loads a PLY, samples it, and normalizes it to unit cube.\"\"\"\n",
    "    try:\n",
    "        # Load Mesh or PCD\n",
    "        # Try mesh first\n",
    "        mesh = o3d.io.read_triangle_mesh(path)\n",
    "        if len(mesh.triangles) > 0:\n",
    "            pcd = mesh.sample_points_uniformly(number_of_points=num_points)\n",
    "            points = np.asarray(pcd.points)\n",
    "        else:\n",
    "            pcd = o3d.io.read_point_cloud(path)\n",
    "            # Basic fallback sampling\n",
    "            if len(pcd.points) > 0:\n",
    "                if len(pcd.points) > num_points:\n",
    "                    pcd = pcd.random_down_sample(num_points / len(pcd.points))\n",
    "                points = np.asarray(pcd.points)\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "        return normalize_to_unit_cube(points)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_group(file_paths, num_points=10000):\n",
    "    \"\"\"Loads a list of file paths into memory as normalized numpy arrays.\"\"\"\n",
    "    data = []\n",
    "    print(f\"Loading {len(file_paths)} files...\")\n",
    "    for f in file_paths:\n",
    "        pts = load_and_process_ply(f, num_points)\n",
    "        if pts is not None and len(pts) > 0:\n",
    "            data.append(pts)\n",
    "    return data\n",
    "\n",
    "# ==========================================\n",
    "# 3. Energy Distance Logic\n",
    "# ==========================================\n",
    "\n",
    "def calculate_energy_distance(group_A, group_B, do_icp=True):\n",
    "    \"\"\"\n",
    "    Computes Energy Distance between two groups of shapes (Group A and Group B).\n",
    "    \n",
    "    Energy Distance D^2(A, B) = 2*E[d(a,b)] - E[d(a,a')] - E[d(b,b')]\n",
    "    \n",
    "    For similarity metrics (F1, IoU, Dice), we convert to distance: d = 1 - score.\n",
    "    For distance metrics (Chamfer, EMD), we use raw values.\n",
    "    \"\"\"\n",
    "    N = len(group_A)\n",
    "    M = len(group_B)\n",
    "    \n",
    "    if N == 0 or M == 0:\n",
    "        print(\"Error: One of the groups is empty.\")\n",
    "        return None\n",
    "\n",
    "    # Initialize accumulation matrices\n",
    "    # We will store: sum, count to calculate means later\n",
    "    keys = ['f1', 'iou', 'dice', 'chamfer', 'emd']\n",
    "    \n",
    "    # Structures to hold raw distances for each component of Energy Distance\n",
    "    # d_XY represents distances between Group A and Group B\n",
    "    # d_XX represents distances within Group A\n",
    "    # d_YY represents distances within Group B\n",
    "    d_XY = {k: [] for k in keys}\n",
    "    d_XX = {k: [] for k in keys}\n",
    "    d_YY = {k: [] for k in keys}\n",
    "\n",
    "    print(\"\\n--- Computing Cross-Distances (A vs B) ---\")\n",
    "    # 1. Compute Cross-Term (A vs B)\n",
    "    # We compute all N*M pairs\n",
    "    count = 0\n",
    "    total_pairs = N * M\n",
    "    for i, p_a in enumerate(group_A):\n",
    "        for j, p_b in enumerate(group_B):\n",
    "            if count % 10 == 0: print(f\"  Processing pair {count}/{total_pairs}...\", end='\\r')\n",
    "            \n",
    "            # Align A to B (or vice versa)\n",
    "            if do_icp:\n",
    "                p_a_aligned = apply_icp(p_a, p_b)\n",
    "            else:\n",
    "                p_a_aligned = p_a\n",
    "                \n",
    "            res = compute_metrics_pair(p_a_aligned, p_b)\n",
    "            \n",
    "            # Store values\n",
    "            # Convert Similarity -> Distance\n",
    "            d_XY['f1'].append(1.0 - res['f1'])\n",
    "            d_XY['iou'].append(1.0 - res['iou'])\n",
    "            d_XY['dice'].append(1.0 - res['dice'])\n",
    "            # Keep Distances as is\n",
    "            d_XY['chamfer'].append(res['chamfer'])\n",
    "            d_XY['emd'].append(res['emd'])\n",
    "            count += 1\n",
    "\n",
    "    print(\"\\n--- Computing Self-Distances (A vs A) ---\")\n",
    "    # 2. Compute Self-Term (A vs A)\n",
    "    # We only need upper triangle for distinct pairs, but Energy Distance usually \n",
    "    # implies full sum. d(x,x) is 0.\n",
    "    count = 0\n",
    "    # Optimization: Only loop i < j (since dist(a,b) ~ dist(b,a)) \n",
    "    # and assume dist(a,a) = 0.\n",
    "    # However, strictly speaking, ICP is not perfectly symmetric. \n",
    "    # For rigorousness, we do full loops or assume symmetry. \n",
    "    # Let's assume symmetry to save 50% time: d(i,j) == d(j,i).\n",
    "    for i in range(N):\n",
    "        for j in range(i + 1, N):\n",
    "            if do_icp:\n",
    "                p_i_aligned = apply_icp(group_A[i], group_A[j])\n",
    "            else:\n",
    "                p_i_aligned = group_A[i]\n",
    "\n",
    "            res = compute_metrics_pair(p_i_aligned, group_A[j])\n",
    "            \n",
    "            val_f1 = 1.0 - res['f1']\n",
    "            val_iou = 1.0 - res['iou']\n",
    "            val_dice = 1.0 - res['dice']\n",
    "            val_cd = res['chamfer']\n",
    "            val_emd = res['emd']\n",
    "            \n",
    "            # Add twice (for i,j and j,i)\n",
    "            d_XX['f1'].extend([val_f1, val_f1])\n",
    "            d_XX['iou'].extend([val_iou, val_iou])\n",
    "            d_XX['dice'].extend([val_dice, val_dice])\n",
    "            d_XX['chamfer'].extend([val_cd, val_cd])\n",
    "            d_XX['emd'].extend([val_emd, val_emd])\n",
    "    \n",
    "    # Add zeros for the diagonal (i=i) if we want technically correct means over N*N\n",
    "    # N zeros for each metric\n",
    "    for k in keys:\n",
    "        d_XX[k].extend([0.0] * N)\n",
    "\n",
    "    print(\"\\n--- Computing Self-Distances (B vs B) ---\")\n",
    "    # 3. Compute Self-Term (B vs B)\n",
    "    for i in range(M):\n",
    "        for j in range(i + 1, M):\n",
    "            if do_icp:\n",
    "                p_i_aligned = apply_icp(group_B[i], group_B[j])\n",
    "            else:\n",
    "                p_i_aligned = group_B[i]\n",
    "\n",
    "            res = compute_metrics_pair(p_i_aligned, group_B[j])\n",
    "            \n",
    "            val_f1 = 1.0 - res['f1']\n",
    "            val_iou = 1.0 - res['iou']\n",
    "            val_dice = 1.0 - res['dice']\n",
    "            val_cd = res['chamfer']\n",
    "            val_emd = res['emd']\n",
    "            \n",
    "            d_YY['f1'].extend([val_f1, val_f1])\n",
    "            d_YY['iou'].extend([val_iou, val_iou])\n",
    "            d_YY['dice'].extend([val_dice, val_dice])\n",
    "            d_YY['chamfer'].extend([val_cd, val_cd])\n",
    "            d_YY['emd'].extend([val_emd, val_emd])\n",
    "            \n",
    "    for k in keys:\n",
    "        d_YY[k].extend([0.0] * M)\n",
    "\n",
    "    # 4. Final Calculation\n",
    "    energy_results = {}\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(f\"ENERGY DISTANCE RESULTS (A: {N} files, B: {M} files)\")\n",
    "    print(\"Interpretation: Lower is more similar distributions.\")\n",
    "    print(\"For F1/IoU/Dice, metric used was (1 - Score).\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "    for k in keys:\n",
    "        # Calculate Expectation (Mean)\n",
    "        E_XY = np.mean(d_XY[k])\n",
    "        E_XX = np.mean(d_XX[k])\n",
    "        E_YY = np.mean(d_YY[k])\n",
    "        \n",
    "        # Energy Distance Formula\n",
    "        ed_value = 2 * E_XY - E_XX - E_YY\n",
    "        \n",
    "        energy_results[k] = ed_value\n",
    "        print(f\"{k.upper():<10} | Energy Dist: {ed_value:.6f} (Avg Cross: {E_XY:.4f}, Avg Intra-A: {E_XX:.4f}, Avg Intra-B: {E_YY:.4f})\")\n",
    "\n",
    "    return energy_results\n",
    "\n",
    "# ==========================================\n",
    "# 4. Main Execution\n",
    "# ==========================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Configuration ---\n",
    "    # Define directories containing PLY files\n",
    "    # Example paths - replace with your actual folders\n",
    "    group_a_dir = \"./data/group_a_shapes\"\n",
    "    group_b_dir = \"./data/group_b_shapes\"\n",
    "    \n",
    "    # Or manual lists\n",
    "    # group_a_files = ['a1.ply', 'a2.ply']\n",
    "    # group_b_files = ['b1.ply', 'b2.ply', 'b3.ply']\n",
    "    \n",
    "    # Just creating dummy data for demonstration if folders don't exist\n",
    "    if not os.path.exists(group_a_dir):\n",
    "        print(\"Note: Example directories not found. Creating dummy spheres for demo...\")\n",
    "        # Create dummy data\n",
    "        def make_sphere(radius, offset):\n",
    "            mesh = o3d.geometry.TriangleMesh.create_sphere(radius=radius)\n",
    "            mesh.translate(offset)\n",
    "            return mesh.sample_points_uniformly(1000)\n",
    "            \n",
    "        # Group A: 3 Small spheres\n",
    "        data_A = [np.asarray(make_sphere(1.0, [0,0,0]).points) for _ in range(3)]\n",
    "        # Group B: 4 Slightly larger spheres (N != M)\n",
    "        data_B = [np.asarray(make_sphere(1.1, [0.1,0,0]).points) for _ in range(4)]\n",
    "        \n",
    "        # Normalize them as the loader usually would\n",
    "        data_A = [normalize_to_unit_cube(p) for p in data_A]\n",
    "        data_B = [normalize_to_unit_cube(p) for p in data_B]\n",
    "    else:\n",
    "        # Real Loading\n",
    "        files_A = glob.glob(os.path.join(group_a_dir, \"*.ply\"))\n",
    "        files_B = glob.glob(os.path.join(group_b_dir, \"*.ply\"))\n",
    "        data_A = load_group(files_A)\n",
    "        data_B = load_group(files_B)\n",
    "\n",
    "    # Run Evaluation\n",
    "    if len(data_A) > 0 and len(data_B) > 0:\n",
    "        start_time = time.time()\n",
    "        results = calculate_energy_distance(data_A, data_B, do_icp=True)\n",
    "        print(f\"\\nTotal calculation time: {time.time() - start_time:.2f} seconds\")\n",
    "    else:\n",
    "        print(\"No data loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "\n",
      "--- Comparing Prediction (PLY) vs GT (NIfTI) ---\n",
      "Pred: 10_CT_HR_1.nii.gz.ply\n",
      "GT:   10_CT_HR_1.nii.gz\n",
      "Loading Prediction PLY...\n",
      "\u001b[1;33m[Open3D WARNING] geometry::TriangleMesh appears to be a geometry::PointCloud (only contains vertices, but no triangles).\u001b[0;m\n",
      "Extracting Surface from GT NIfTI...\n",
      "Warning: NIfTI volume is empty (all values below threshold).\n",
      "Error: One of the inputs yielded 0 points.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import nibabel as nib\n",
    "from skimage import measure  # For Marching Cubes\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import os\n",
    "\n",
    "# ==========================================\n",
    "# 1. Metric Helper Functions (Preserved)\n",
    "# ==========================================\n",
    "\n",
    "def normalize_to_unit_cube(points):\n",
    "    \"\"\"Normalizes point cloud to [-1, 1] range.\"\"\"\n",
    "    if len(points) == 0: return points\n",
    "    centroid = np.mean(points, axis=0)\n",
    "    points_centered = points - centroid\n",
    "    max_dist = np.max(np.abs(points_centered))\n",
    "    return points_centered / max_dist if max_dist > 0 else points_centered\n",
    "\n",
    "def apply_icp(source_points, target_points, threshold=0.02):\n",
    "    \"\"\"Aligns source points to target points using ICP.\"\"\"\n",
    "    if len(source_points) == 0 or len(target_points) == 0:\n",
    "        return source_points\n",
    "    \n",
    "    source_pcd = o3d.geometry.PointCloud()\n",
    "    source_pcd.points = o3d.utility.Vector3dVector(source_points)\n",
    "    target_pcd = o3d.geometry.PointCloud()\n",
    "    target_pcd.points = o3d.utility.Vector3dVector(target_points)\n",
    "    \n",
    "    trans_init = np.identity(4)\n",
    "    reg_p2p = o3d.pipelines.registration.registration_icp(\n",
    "        source_pcd, target_pcd, threshold, trans_init,\n",
    "        o3d.pipelines.registration.TransformationEstimationPointToPoint()\n",
    "    )\n",
    "    source_pcd.transform(reg_p2p.transformation)\n",
    "    return np.asarray(source_pcd.points)\n",
    "\n",
    "def compute_f1_score(pred_points, gt_points, threshold=0.01):\n",
    "    \"\"\"Computes F1@0.01 score.\"\"\"\n",
    "    if len(pred_points) == 0 or len(gt_points) == 0: return 0.0, 0.0, 0.0\n",
    "    \n",
    "    gt_tree = cKDTree(gt_points)\n",
    "    pred_tree = cKDTree(pred_points)\n",
    "    \n",
    "    dist_pred_to_gt, _ = gt_tree.query(pred_points, k=1)\n",
    "    precision = np.mean(dist_pred_to_gt < threshold)\n",
    "    \n",
    "    dist_gt_to_pred, _ = pred_tree.query(gt_points, k=1)\n",
    "    recall = np.mean(dist_gt_to_pred < threshold)\n",
    "    \n",
    "    if precision + recall == 0: return 0.0, precision, recall\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1, precision, recall\n",
    "\n",
    "def compute_voxel_iou(point_cloud_1, point_cloud_2, resolution=64):\n",
    "    \"\"\"Computes Voxel-IoU at 64^3 resolution.\"\"\"\n",
    "    if len(point_cloud_1) == 0 and len(point_cloud_2) == 0: return 1.0\n",
    "    if len(point_cloud_1) == 0 or len(point_cloud_2) == 0: return 0.0\n",
    "    \n",
    "    min_bound = np.array([-1.0, -1.0, -1.0])\n",
    "    max_bound = np.array([1.0, 1.0, 1.0])\n",
    "    voxel_size = (max_bound - min_bound) / resolution\n",
    "    \n",
    "    def get_voxels(points):\n",
    "        indices = np.floor((points - min_bound) / voxel_size).astype(int)\n",
    "        indices = np.clip(indices, 0, resolution - 1)\n",
    "        return set(map(tuple, indices))\n",
    "\n",
    "    voxels_1 = get_voxels(point_cloud_1)\n",
    "    voxels_2 = get_voxels(point_cloud_2)\n",
    "    \n",
    "    intersection = len(voxels_1.intersection(voxels_2))\n",
    "    union = len(voxels_1.union(voxels_2))\n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "def compute_voxel_dice(point_cloud_1, point_cloud_2, resolution=64):\n",
    "    \"\"\"Computes Voxel-Dice at 64^3 resolution.\"\"\"\n",
    "    if len(point_cloud_1) == 0 and len(point_cloud_2) == 0: return 1.0\n",
    "    if len(point_cloud_1) == 0 or len(point_cloud_2) == 0: return 0.0\n",
    "    \n",
    "    min_bound = np.array([-1.0, -1.0, -1.0])\n",
    "    max_bound = np.array([1.0, 1.0, 1.0])\n",
    "    voxel_size = (max_bound - min_bound) / resolution\n",
    "    \n",
    "    def get_voxels(points):\n",
    "        indices = np.floor((points - min_bound) / voxel_size).astype(int)\n",
    "        indices = np.clip(indices, 0, resolution - 1)\n",
    "        return set(map(tuple, indices))\n",
    "\n",
    "    voxels_1 = get_voxels(point_cloud_1)\n",
    "    voxels_2 = get_voxels(point_cloud_2)\n",
    "    \n",
    "    intersection = len(voxels_1.intersection(voxels_2))\n",
    "    denom = len(voxels_1) + len(voxels_2)\n",
    "    \n",
    "    return (2.0 * intersection) / denom if denom > 0 else 0.0\n",
    "\n",
    "def compute_chamfer_distance(pred_points, gt_points):\n",
    "    \"\"\"Computes Chamfer Distance (CD).\"\"\"\n",
    "    if len(pred_points) == 0 or len(gt_points) == 0: return float('inf')\n",
    "    \n",
    "    gt_tree = cKDTree(gt_points)\n",
    "    pred_tree = cKDTree(pred_points)\n",
    "    \n",
    "    dist_pred_to_gt, _ = gt_tree.query(pred_points, k=1)\n",
    "    dist_gt_to_pred, _ = pred_tree.query(gt_points, k=1)\n",
    "    \n",
    "    return np.mean(dist_pred_to_gt) + np.mean(dist_gt_to_pred)\n",
    "\n",
    "def compute_emd(pred_points, gt_points, max_points=2048):\n",
    "    \"\"\"Computes Earth Mover's Distance (EMD).\"\"\"\n",
    "    if len(pred_points) == 0 or len(gt_points) == 0:\n",
    "        return float('inf')\n",
    "\n",
    "    n_points = min(len(pred_points), len(gt_points), max_points)\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    indices_pred = np.random.choice(len(pred_points), n_points, replace=False)\n",
    "    indices_gt = np.random.choice(len(gt_points), n_points, replace=False)\n",
    "    \n",
    "    pred_sampled = pred_points[indices_pred]\n",
    "    gt_sampled = gt_points[indices_gt]\n",
    "    \n",
    "    dists = cdist(pred_sampled, gt_sampled, metric='euclidean')\n",
    "    row_ind, col_ind = linear_sum_assignment(dists)\n",
    "    \n",
    "    total_cost = dists[row_ind, col_ind].sum()\n",
    "    emd_value = total_cost / n_points\n",
    "    \n",
    "    return emd_value\n",
    "\n",
    "# ==========================================\n",
    "# 2. Data Extraction Helpers (Updated for NII)\n",
    "# ==========================================\n",
    "\n",
    "def load_and_sample_ply(ply_input, num_points=10000):\n",
    "    \"\"\"Loads a PLY file and samples points.\"\"\"\n",
    "    if isinstance(ply_input, str):\n",
    "        if not os.path.exists(ply_input):\n",
    "            print(f\"Error: PLY file not found at {ply_input}\")\n",
    "            return np.zeros((0, 3))\n",
    "        try:\n",
    "            mesh = o3d.io.read_triangle_mesh(ply_input)\n",
    "            if len(mesh.triangles) > 0:\n",
    "                pcd = mesh.sample_points_uniformly(number_of_points=num_points)\n",
    "                return np.asarray(pcd.points)\n",
    "            else:\n",
    "                pcd = o3d.io.read_point_cloud(ply_input)\n",
    "                if len(pcd.points) > num_points:\n",
    "                    ratio = num_points / len(pcd.points)\n",
    "                    pcd = pcd.random_down_sample(ratio)\n",
    "                return np.asarray(pcd.points)\n",
    "        except Exception as e:\n",
    "            print(f\"PLY Loading Error: {e}\")\n",
    "            return np.zeros((0, 3))\n",
    "    return np.zeros((0, 3))\n",
    "\n",
    "def load_nii_as_surface_points(nii_path, num_points=10000, iso_value=0.5):\n",
    "    \"\"\"\n",
    "    Loads a NIfTI file, extracts the isosurface (Marching Cubes), \n",
    "    and samples points from that surface.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(nii_path):\n",
    "        print(f\"Error: NIfTI file not found at {nii_path}\")\n",
    "        return np.zeros((0, 3))\n",
    "    \n",
    "    try:\n",
    "        # 1. Load NIfTI\n",
    "        nii = nib.load(nii_path)\n",
    "        data = nii.get_fdata()\n",
    "        \n",
    "        # 2. Check if empty\n",
    "        if data.max() < iso_value:\n",
    "            print(\"Warning: NIfTI volume is empty (all values below threshold).\")\n",
    "            return np.zeros((0, 3))\n",
    "\n",
    "        # 3. Marching Cubes to get surface vertices (in index coordinates)\n",
    "        # We assume the mask is binary or has a clear threshold\n",
    "        verts, faces, _, _ = measure.marching_cubes(data, level=iso_value)\n",
    "        \n",
    "        # 4. Apply voxel spacing (zooms) to preserve aspect ratio\n",
    "        # We do NOT apply full affine (translation/rotation) usually, because\n",
    "        # we are going to normalize to unit cube anyway. But scaling matters.\n",
    "        zooms = nii.header.get_zooms()[:3]\n",
    "        verts = verts * np.array(zooms)\n",
    "        \n",
    "        # 5. Convert to Open3D Mesh for easy sampling\n",
    "        mesh = o3d.geometry.TriangleMesh()\n",
    "        mesh.vertices = o3d.utility.Vector3dVector(verts)\n",
    "        mesh.triangles = o3d.utility.Vector3iVector(faces)\n",
    "        \n",
    "        # 6. Sample Points\n",
    "        pcd = mesh.sample_points_uniformly(number_of_points=num_points)\n",
    "        return np.asarray(pcd.points)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"NIfTI Processing Error: {e}\")\n",
    "        return np.zeros((0, 3))\n",
    "\n",
    "# ==========================================\n",
    "# 3. Main Comparison Function (PLY vs NII)\n",
    "# ==========================================\n",
    "\n",
    "def evaluate_ply_vs_nii(pred_ply_path, gt_nii_path, num_points=10000, do_icp=True):\n",
    "    \"\"\"\n",
    "    Compares a generated PLY against a Ground Truth NIfTI.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Comparing Prediction (PLY) vs GT (NIfTI) ---\")\n",
    "    print(f\"Pred: {os.path.basename(pred_ply_path)}\")\n",
    "    print(f\"GT:   {os.path.basename(gt_nii_path)}\")\n",
    "    \n",
    "    # 1. Extraction\n",
    "    print(\"Loading Prediction PLY...\")\n",
    "    pred_points = load_and_sample_ply(pred_ply_path, num_points=num_points)\n",
    "    \n",
    "    print(\"Extracting Surface from GT NIfTI...\")\n",
    "    gt_points = load_nii_as_surface_points(gt_nii_path, num_points=num_points)\n",
    "    \n",
    "    # Handle Missing/Empty\n",
    "    if len(pred_points) == 0 or len(gt_points) == 0:\n",
    "        print(\"Error: One of the inputs yielded 0 points.\")\n",
    "        return\n",
    "        \n",
    "    print(f\"Loaded points -> Pred: {len(pred_points)}, GT (Surface): {len(gt_points)}\")\n",
    "\n",
    "    # 2. Normalize \n",
    "    # This is crucial because NIfTI is in mm/voxels and PLY might be in arbitrary units\n",
    "    gt_norm = normalize_to_unit_cube(gt_points)\n",
    "    pred_norm = normalize_to_unit_cube(pred_points)\n",
    "    \n",
    "    # 3. Align (ICP)\n",
    "    if do_icp:\n",
    "        print(\"Applying ICP alignment...\")\n",
    "        # Align Pred to GT\n",
    "        pred_aligned = apply_icp(pred_norm, gt_norm, threshold=0.05)\n",
    "    else:\n",
    "        pred_aligned = pred_norm\n",
    "    \n",
    "    # 4. Compute Metrics\n",
    "    print(\"Computing Metrics...\")\n",
    "    f1, precision, recall = compute_f1_score(pred_aligned, gt_norm)\n",
    "    iou = compute_voxel_iou(pred_aligned, gt_norm)\n",
    "    dice = compute_voxel_dice(pred_aligned, gt_norm)\n",
    "    chamfer = compute_chamfer_distance(pred_aligned, gt_norm)\n",
    "    emd = compute_emd(pred_aligned, gt_norm, max_points=2048)\n",
    "    \n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Results: {os.path.basename(pred_ply_path)} vs {os.path.basename(gt_nii_path)}\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"  F1@0.01:   {f1:.4f}\")\n",
    "    print(f\"  Voxel-IoU: {iou:.4f}\")\n",
    "    print(f\"  Voxel-Dice:{dice:.4f}\")\n",
    "    print(f\"  Chamfer:   {chamfer:.4f}\")\n",
    "    print(f\"  EMD:       {emd:.4f}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Update these paths to your actual files\n",
    "    pred_ply = '/PHShome/yl535/project/python/sam_3d/sam-3d-objects/results_AeroPath/lungs/10_CT_HR_1.nii.gz.ply'\n",
    "    gt_nii   = '/PHShome/yl535/project/python/datasets/AeroPath/lungs_segmented/10_CT_HR_1_mask.nii.gz'\n",
    "\n",
    "    # Check if files exist before running to avoid immediate errors\n",
    "    if os.path.exists(pred_ply) and os.path.exists(gt_nii):\n",
    "        evaluate_ply_vs_nii(pred_ply, gt_nii)\n",
    "    else:\n",
    "        print(\"Please set valid paths in the __main__ block.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Inspecting PLY: 10_CT_HR_1.nii.gz.ply ---\n",
      "\u001b[1;33m[Open3D WARNING] geometry::TriangleMesh appears to be a geometry::PointCloud (only contains vertices, but no triangles).\u001b[0;m\n",
      "  [As Mesh] Vertices: 206720\n",
      "  [As Mesh] Triangles: 0\n",
      "  WARNING: No triangles found. This is likely a Point Cloud, not a Mesh.\n",
      "  [As PCD]  Points: 206720\n",
      " PLY contains valid points.\n",
      "\n",
      "--- Inspecting NIfTI: 10_CT_HR_1.nii.gz ---\n",
      "  Shape: (485, 337, 219)\n",
      "  Data Type: float64\n",
      "  Min Value: -1024.0\n",
      "  Max Value: 0.0\n",
      "  Unique Values (first 10): [-1024. -1023. -1022. -1021. -1020. -1019. -1018. -1017. -1016. -1015.]\n",
      " ERROR: Max value is <= 0. The volume appears empty.\n"
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def inspect_nifti(nii_path):\n",
    "    print(f\"\\n--- Inspecting NIfTI: {os.path.basename(nii_path)} ---\")\n",
    "    if not os.path.exists(nii_path):\n",
    "        print(f\" File not found at: {nii_path}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        nii = nib.load(nii_path)\n",
    "        data = nii.get_fdata()\n",
    "        \n",
    "        print(f\"  Shape: {data.shape}\")\n",
    "        print(f\"  Data Type: {data.dtype}\")\n",
    "        print(f\"  Min Value: {data.min()}\")\n",
    "        print(f\"  Max Value: {data.max()}\")\n",
    "        print(f\"  Unique Values (first 10): {np.unique(data)[:10]}\")\n",
    "        \n",
    "        # Check for empty mask\n",
    "        if data.max() <= 0:\n",
    "            print(\" ERROR: Max value is <= 0. The volume appears empty.\")\n",
    "        elif data.max() < 0.5:\n",
    "            print(f\"  WARNING: Max value ({data.max()}) is < 0.5. 'marching_cubes' at level=0.5 will fail.\")\n",
    "        else:\n",
    "            print(\" Data looks valid for isosurface extraction.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\" Error loading NIfTI: {e}\")\n",
    "\n",
    "def inspect_ply(ply_path):\n",
    "    print(f\"\\n--- Inspecting PLY: {os.path.basename(ply_path)} ---\")\n",
    "    if not os.path.exists(ply_path):\n",
    "        print(f\" File not found at: {ply_path}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Attempt 1: Load as Mesh (requires triangles)\n",
    "        mesh = o3d.io.read_triangle_mesh(ply_path)\n",
    "        if mesh.has_vertices():\n",
    "            print(f\"  [As Mesh] Vertices: {len(mesh.vertices)}\")\n",
    "            print(f\"  [As Mesh] Triangles: {len(mesh.triangles)}\")\n",
    "            if len(mesh.triangles) == 0:\n",
    "                print(\"  WARNING: No triangles found. This is likely a Point Cloud, not a Mesh.\")\n",
    "        else:\n",
    "            print(\"  [As Mesh] Failed to load vertices.\")\n",
    "\n",
    "        # Attempt 2: Load as Point Cloud\n",
    "        pcd = o3d.io.read_point_cloud(ply_path)\n",
    "        if pcd.has_points():\n",
    "            print(f\"  [As PCD]  Points: {len(pcd.points)}\")\n",
    "            if len(pcd.points) > 0:\n",
    "                print(\" PLY contains valid points.\")\n",
    "        else:\n",
    "            print(\" ERROR: PLY appears empty (0 points).\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error loading PLY: {e}\")\n",
    "\n",
    "# --- UPDATE THESE PATHS ---\n",
    "bad_pred_ply = '/PHShome/yl535/project/python/sam_3d/sam-3d-objects/results_AeroPath/lungs/10_CT_HR_1.nii.gz.ply'\n",
    "bad_gt_nii   = '/PHShome/yl535/project/python/datasets/AeroPath/lungs_segmented/10_CT_HR_1.nii.gz'\n",
    "\n",
    "\n",
    "\n",
    "inspect_ply(bad_pred_ply)\n",
    "inspect_nifti(bad_gt_nii)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam3d-objects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
