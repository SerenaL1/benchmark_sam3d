{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Copyright (c) Meta Platforms, Inc. and affiliates."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Imports and Model Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/shared/ssd_28T/home/yl535/anaconda3/envs/sam3d-objects/lib/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
                        "  import pynvml  # type: ignore[import]\n",
                        "Warp CUDA error 2: out of memory (in function wp_cuda_device_get_memory_info, /builds/omniverse/warp/warp/native/warp.cu:2231)\n",
                        "Warp CUDA error 201: invalid device context (in function wp_cuda_device_get_memory_info, /builds/omniverse/warp/warp/native/warp.cu:2234)\n",
                        "Warp CUDA error 201: invalid device context (in function wp_cuda_device_get_memory_info, /builds/omniverse/warp/warp/native/warp.cu:2236)\n",
                        "Warp CUDA error 2: out of memory (in function wp_cuda_device_get_memory_info, /builds/omniverse/warp/warp/native/warp.cu:2231)\n",
                        "Warp CUDA error 201: invalid device context (in function wp_cuda_device_get_memory_info, /builds/omniverse/warp/warp/native/warp.cu:2234)\n",
                        "Warp CUDA error 201: invalid device context (in function wp_cuda_device_get_memory_info, /builds/omniverse/warp/warp/native/warp.cu:2236)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Warp 1.10.0 initialized:\n",
                        "   CUDA Toolkit 12.8, Driver 12.2\n",
                        "   Devices:\n",
                        "     \"cpu\"      : \"x86_64\"\n",
                        "     \"cuda:0\"   : \"NVIDIA H100 80GB HBM3\" (79 GiB, sm_90, mempool enabled)\n",
                        "     \"cuda:1\"   : \"NVIDIA H100 80GB HBM3\" (0 GiB, sm_90, mempool enabled)\n",
                        "     \"cuda:2\"   : \"NVIDIA H100 80GB HBM3\" (0 GiB, sm_90, mempool enabled)\n",
                        "     \"cuda:3\"   : \"NVIDIA H100 80GB HBM3\" (79 GiB, sm_90, mempool enabled)\n",
                        "     \"cuda:4\"   : \"NVIDIA H100 80GB HBM3\" (79 GiB, sm_90, mempool enabled)\n",
                        "     \"cuda:5\"   : \"NVIDIA H100 80GB HBM3\" (79 GiB, sm_90, mempool enabled)\n",
                        "     \"cuda:6\"   : \"NVIDIA H100 80GB HBM3\" (79 GiB, sm_90, mempool enabled)\n",
                        "     \"cuda:7\"   : \"NVIDIA H100 80GB HBM3\" (79 GiB, sm_90, mempool enabled)\n",
                        "   CUDA peer access:\n",
                        "     Supported fully (all-directional)\n",
                        "   Kernel cache:\n",
                        "     /PHShome/yl535/.cache/warp/1.10.0\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[32m2025-11-21 11:04:54.850\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.pipeline.inference_pipeline\u001b[0m:\u001b[36mset_attention_backend\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mGPU name is NVIDIA H100 80GB HBM3\u001b[0m\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
                        "[Open3D INFO] WebRTC GUI backend enabled.\n",
                        "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[32m2025-11-21 11:04:57.475\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.model.backbone.tdfy_dit.modules.sparse\u001b[0m:\u001b[36m__from_env\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1m[SPARSE] Backend: spconv, Attention: flash_attn\u001b[0m\n",
                        "\u001b[32m2025-11-21 11:05:02.625\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.model.backbone.tdfy_dit.modules.attention\u001b[0m:\u001b[36m__from_env\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1m[ATTENTION] Using backend: flash_attn\u001b[0m\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[SPARSE][CONV] spconv algo: auto\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[32m2025-11-21 11:05:03.339\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36msam3d_objects.data.dataset.tdfy.preprocessor\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m51\u001b[0m - \u001b[33m\u001b[1mNo rgb pointmap normalizer provided, using scale + shift \u001b[0m\n",
                        "\u001b[32m2025-11-21 11:05:03.340\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36msam3d_objects.data.dataset.tdfy.preprocessor\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m51\u001b[0m - \u001b[33m\u001b[1mNo rgb pointmap normalizer provided, using scale + shift \u001b[0m\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import imageio\n",
                "import uuid\n",
                "from IPython.display import Image as ImageDisplay\n",
                "from inference import Inference, ready_gaussian_for_video_rendering, render_video, load_image, load_single_mask, display_image, make_scene, interactive_visualizer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[32m2025-11-21 11:05:16.188\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36msam3d_objects.data.dataset.tdfy.preprocessor\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m51\u001b[0m - \u001b[33m\u001b[1mNo rgb pointmap normalizer provided, using scale + shift \u001b[0m\n",
                        "/shared/ssd_28T/home/yl535/anaconda3/envs/sam3d-objects/lib/python3.11/site-packages/moge/model/v1.py:171: UserWarning: The following deprecated/invalid arguments are ignored: {'output_mask': True, 'split_head': True}\n",
                        "  warnings.warn(f\"The following deprecated/invalid arguments are ignored: {deprecated_kwargs}\")\n",
                        "\u001b[32m2025-11-21 11:05:39.658\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36msam3d_objects.data.dataset.tdfy.preprocessor\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m51\u001b[0m - \u001b[33m\u001b[1mNo rgb pointmap normalizer provided, using scale + shift \u001b[0m\n",
                        "\u001b[32m2025-11-21 11:05:39.664\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.pipeline.inference_pipeline\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m98\u001b[0m - \u001b[1mself.device: cuda\u001b[0m\n",
                        "\u001b[32m2025-11-21 11:05:39.664\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.pipeline.inference_pipeline\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m99\u001b[0m - \u001b[1mCUDA_VISIBLE_DEVICES: None\u001b[0m\n",
                        "\u001b[32m2025-11-21 11:05:39.665\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.pipeline.inference_pipeline\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m100\u001b[0m - \u001b[1mActually using GPU: 0\u001b[0m\n",
                        "\u001b[32m2025-11-21 11:05:39.665\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.pipeline.inference_pipeline\u001b[0m:\u001b[36minit_pose_decoder\u001b[0m:\u001b[36m295\u001b[0m - \u001b[1mUsing pose decoder: ScaleShiftInvariant\u001b[0m\n",
                        "\u001b[32m2025-11-21 11:05:39.666\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.pipeline.inference_pipeline\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m131\u001b[0m - \u001b[1mLoading model weights...\u001b[0m\n",
                        "\u001b[32m2025-11-21 11:05:40.971\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.model.io\u001b[0m:\u001b[36mload_model_from_checkpoint\u001b[0m:\u001b[36m158\u001b[0m - \u001b[1mLoading checkpoint from /PHShome/yl535/project/python/sam_3d/sam-3d-objects/checkpoints/checkpoints/ss_generator.ckpt\u001b[0m\n",
                        "\u001b[32m2025-11-21 11:05:50.793\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.model.io\u001b[0m:\u001b[36mload_model_from_checkpoint\u001b[0m:\u001b[36m158\u001b[0m - \u001b[1mLoading checkpoint from /PHShome/yl535/project/python/sam_3d/sam-3d-objects/checkpoints/checkpoints/slat_generator.ckpt\u001b[0m\n",
                        "\u001b[32m2025-11-21 11:05:59.678\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.model.io\u001b[0m:\u001b[36mload_model_from_checkpoint\u001b[0m:\u001b[36m158\u001b[0m - \u001b[1mLoading checkpoint from /PHShome/yl535/project/python/sam_3d/sam-3d-objects/checkpoints/checkpoints/ss_decoder.ckpt\u001b[0m\n",
                        "\u001b[32m2025-11-21 11:06:00.201\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.model.io\u001b[0m:\u001b[36mload_model_from_checkpoint\u001b[0m:\u001b[36m158\u001b[0m - \u001b[1mLoading checkpoint from /PHShome/yl535/project/python/sam_3d/sam-3d-objects/checkpoints/checkpoints/slat_decoder_gs.ckpt\u001b[0m\n",
                        "\u001b[32m2025-11-21 11:06:00.363\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.model.io\u001b[0m:\u001b[36mload_model_from_checkpoint\u001b[0m:\u001b[36m158\u001b[0m - \u001b[1mLoading checkpoint from /PHShome/yl535/project/python/sam_3d/sam-3d-objects/checkpoints/checkpoints/slat_decoder_gs_4.ckpt\u001b[0m\n",
                        "\u001b[32m2025-11-21 11:06:00.741\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.model.io\u001b[0m:\u001b[36mload_model_from_checkpoint\u001b[0m:\u001b[36m158\u001b[0m - \u001b[1mLoading checkpoint from /PHShome/yl535/project/python/sam_3d/sam-3d-objects/checkpoints/checkpoints/slat_decoder_mesh.ckpt\u001b[0m\n",
                        "\u001b[32m2025-11-21 11:06:02.365\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.model.backbone.dit.embedder.dino\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mLoading DINO model: dinov2_vitl14_reg from facebookresearch/dinov2 (source: github)\u001b[0m\n",
                        "\u001b[32m2025-11-21 11:06:03.518\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.model.backbone.dit.embedder.dino\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m44\u001b[0m - \u001b[1mLoaded DINO model - type: <class 'dinov2.models.vision_transformer.DinoVisionTransformer'>, embed_dim: 1024, patch_size: (14, 14)\u001b[0m\n",
                        "\u001b[32m2025-11-21 11:06:03.524\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.model.backbone.dit.embedder.dino\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mLoading DINO model: dinov2_vitl14_reg from facebookresearch/dinov2 (source: github)\u001b[0m\n",
                        "\u001b[32m2025-11-21 11:06:04.795\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.model.backbone.dit.embedder.dino\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m44\u001b[0m - \u001b[1mLoaded DINO model - type: <class 'dinov2.models.vision_transformer.DinoVisionTransformer'>, embed_dim: 1024, patch_size: (14, 14)\u001b[0m\n",
                        "\u001b[32m2025-11-21 11:06:04.860\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.model.io\u001b[0m:\u001b[36mload_model_from_checkpoint\u001b[0m:\u001b[36m158\u001b[0m - \u001b[1mLoading checkpoint from /PHShome/yl535/project/python/sam_3d/sam-3d-objects/checkpoints/checkpoints/ss_generator.ckpt\u001b[0m\n",
                        "\u001b[32m2025-11-21 11:06:10.457\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.model.backbone.dit.embedder.dino\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mLoading DINO model: dinov2_vitl14_reg from facebookresearch/dinov2 (source: github)\u001b[0m\n",
                        "\u001b[32m2025-11-21 11:06:11.413\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.model.backbone.dit.embedder.dino\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m44\u001b[0m - \u001b[1mLoaded DINO model - type: <class 'dinov2.models.vision_transformer.DinoVisionTransformer'>, embed_dim: 1024, patch_size: (14, 14)\u001b[0m\n",
                        "\u001b[32m2025-11-21 11:06:11.419\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.model.backbone.dit.embedder.dino\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mLoading DINO model: dinov2_vitl14_reg from facebookresearch/dinov2 (source: github)\u001b[0m\n",
                        "\u001b[32m2025-11-21 11:06:12.171\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.model.backbone.dit.embedder.dino\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m44\u001b[0m - \u001b[1mLoaded DINO model - type: <class 'dinov2.models.vision_transformer.DinoVisionTransformer'>, embed_dim: 1024, patch_size: (14, 14)\u001b[0m\n",
                        "\u001b[32m2025-11-21 11:06:12.178\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.model.io\u001b[0m:\u001b[36mload_model_from_checkpoint\u001b[0m:\u001b[36m158\u001b[0m - \u001b[1mLoading checkpoint from /PHShome/yl535/project/python/sam_3d/sam-3d-objects/checkpoints/checkpoints/slat_generator.ckpt\u001b[0m\n",
                        "\u001b[32m2025-11-21 11:06:16.102\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.pipeline.inference_pipeline\u001b[0m:\u001b[36moverride_ss_generator_cfg_config\u001b[0m:\u001b[36m436\u001b[0m - \u001b[1mss_generator parameters: inference_steps=25, cfg_strength=7, cfg_interval=[0, 500], rescale_t=3, cfg_strength_pm=0.0\u001b[0m\n",
                        "\u001b[32m2025-11-21 11:06:16.105\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.pipeline.inference_pipeline\u001b[0m:\u001b[36moverride_slat_generator_cfg_config\u001b[0m:\u001b[36m458\u001b[0m - \u001b[1mslat_generator parameters: inference_steps=25, cfg_strength=1, cfg_interval=[0, 500], rescale_t=1\u001b[0m\n",
                        "\u001b[32m2025-11-21 11:06:16.106\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msam3d_objects.pipeline.inference_pipeline\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m196\u001b[0m - \u001b[1mLoading model weights completed!\u001b[0m\n"
                    ]
                }
            ],
            "source": [
                "PATH = os.getcwd()\n",
                "TAG = \"hf\"\n",
                "config_path = f\"{PATH}/../checkpoints/{TAG}/pipeline.yaml\"\n",
                "config_path = f\"{PATH}/../checkpoints/checkpoints/pipeline.yaml\"\n",
                "inference = Inference(config_path, compile=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load input image to lift to 3D (single object)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "IMAGE_PATH = f\"{PATH}/images/shutterstock_stylish_kidsroom_1640806567/image.png\"\n",
                "IMAGE_NAME = os.path.basename(os.path.dirname(IMAGE_PATH))\n",
                "\n",
                "image = load_image(IMAGE_PATH)\n",
                "mask = load_single_mask(os.path.dirname(IMAGE_PATH), index=14)\n",
                "display_image(image, masks=[mask])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Generate Gaussian Splat"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# run model\n",
                "output = inference(image, mask, seed=42)\n",
                "\n",
                "# export gaussian splat (as point cloud)\n",
                "output[\"gs\"].save_ply(f\"{PATH}/gaussians/single/{IMAGE_NAME}.ply\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Visualize Gaussian Splat\n",
                "### a. Animated Gif"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# render gaussian splat\n",
                "scene_gs = make_scene(output)\n",
                "scene_gs = ready_gaussian_for_video_rendering(scene_gs)\n",
                "\n",
                "video = render_video(\n",
                "    scene_gs,\n",
                "    r=1,\n",
                "    fov=60,\n",
                "    pitch_deg=15,\n",
                "    yaw_start_deg=-45,\n",
                "    resolution=512,\n",
                ")[\"color\"]\n",
                "\n",
                "# save video as gif\n",
                "imageio.mimsave(\n",
                "    os.path.join(f\"{PATH}/gaussians/single/{IMAGE_NAME}.gif\"),\n",
                "    video,\n",
                "    format=\"GIF\",\n",
                "    duration=1000 / 30,  # default assuming 30fps from the input MP4\n",
                "    loop=0,  # 0 means loop indefinitely\n",
                ")\n",
                "\n",
                "# notebook display\n",
                "ImageDisplay(url=f\"gaussians/single/{IMAGE_NAME}.gif?cache_invalidator={uuid.uuid4()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### b. Interactive Visualizer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# might take a while to load (black screen)\n",
                "interactive_visualizer(f\"{PATH}/gaussians/single/{IMAGE_NAME}.ply\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "sam3d-objects",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
